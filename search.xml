<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LXP广场</title>
    <url>/2020/03/19/%E7%95%99%E8%A8%80%E6%9D%BF/</url>
    <content><![CDATA[<p>现在我宣布，这里是LXP广场！<br><a id="more"></a><br><img src="/2020/03/19/%E7%95%99%E8%A8%80%E6%9D%BF/2.jpg" alt="logo"></p>
]]></content>
  </entry>
  <entry>
    <title>最优估计学习备忘四</title>
    <url>/2020/03/19/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E5%9B%9B/</url>
    <content><![CDATA[<h1 id="线性卡尔曼滤波的实施"><a href="#线性卡尔曼滤波的实施" class="headerlink" title="线性卡尔曼滤波的实施"></a>线性卡尔曼滤波的实施</h1><p>在理想的情况下，如果数学模型与物理现实一致，卡尔曼滤波得到的是无偏线性最小方差估计，而且随着测量数据的增加，卡尔曼滤波估计逐渐趋近状态的期望，误差方差逐渐趋于稳态值。但在实际应用中卡尔曼滤波会出现各种问题，最常见的是滤波估计值偏离实际的状态值，如果随着时间的推移这种偏离越来越大甚至趋于无穷大，就称滤波发生了发散。同时一般在滤波估计发生偏移和发散时，滤波估计的方差也发生偏移和发散。<br><a id="more"></a><br>滤波发散的原因有三个，一是模型发散，即系统的数学模型不是一致完全可控和一致完全可测的，滤波不具有一致渐进稳定性，抑制模型发散的基本方法是改变系统的结构及参数，使系统在随机作用下有一致完全可控可测；二是数值发散，由于计算机的字长是有限的，这就使得滤波递推中的每一步计算都有截断误差，这使得误差方差逐渐失去正定性甚至对称性，滤波增益的计算值与理论值的差异越来越大，从而导致发散，抑制数值发散的最基本方法是采用双字长运算，这样可以有效减少数字的损失，对于滤波问题，抑制数值发散还可以在滤波过程中将方差阵分解（使用平方根滤波、$UDU^T$分解滤波），通过分解后的矩阵代替原来的方差矩阵进行递推，此外当滤波的方差很大甚至无穷大时，将导致滤波无法计算，这时可将方差的逆矩阵来代替原方差进行滤波递推（信息滤波）；三是由于对物理现实认识不足，对实际系统的过程缺乏完整的了解或足够的统计数据，是建立了不准确的数学模型，针对此类问题，可使用有色噪声下卡尔曼滤波（将有色噪声转化为白噪声）、扩展的卡尔曼滤波（一定程度上减小方程线性化带来的模型误差）、自适应的卡尔曼滤波（实时地调整和修正系统模型和噪声地方差矩阵，以达到模型地自适应地与现实相吻合）来解决对应的问题。</p>
<h2 id="平方根滤波"><a href="#平方根滤波" class="headerlink" title="平方根滤波"></a>平方根滤波</h2><p>由于计算机舍入误差的存在，使得滤波方差矩阵$D_{\hat{X}}(k,k-1)$和$D_{\hat{X}}(k)$的数值失去非负定性甚至失去对称性，从而导致$K_k$的计算失真，并产生发散现象。计算误差主要是由于过大的数值与过小的数值相加减过程中造成的，为了避免运算过程中的数值过大或过小，我们可以将其平方根分解（对数值开方，例如0.01开方成0.1）。在卡尔曼滤波递推计算中，我们可以对方差矩阵做平方根分解，对称正定矩阵$D$可以唯一地分解为一个下三角矩阵$S$及其转置$S^T$的乘积，即$D=SS^T$，矩阵$S$即为矩阵$D$的平方根矩阵。在计算矩阵$S$只需要保留矩阵$D$一半的字长，$SS^T$就可以达到很高的精度。正定矩阵可使用$Cholesky$分解法。</p>
<p>首先将状态方差矩阵做平方根分解：</p>
<script type="math/tex; mode=display">
D_{\hat{X}}=S_kS_k^T，D_{\hat{X}}(k,k-1)=S_{k,k-1}S_{k,k-1}^T</script><p>然后在卡尔曼滤波的基本方程中，以$S_k$和$S_{k.k-1}$的递推关系式来代替原来的$D_{\hat{X}}$和$D_{\hat{X}}(k,k-1)$</p>
<p>的递推关系式，这样就可以保证方差矩阵在任意时刻都是对称非负定的。这里不加推导地给出平方根滤波递推公式</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>一步预测</th>
<th>状态预测</th>
<th>$\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)$</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>预测方差</td>
<td>$D_{\hat{X}}(k,k-1)=S_{k,k-1}S_{k,k-1}^T$</td>
</tr>
<tr>
<td>测量更新</td>
<td>状态滤波</td>
<td>$\begin{cases}\hat{X}^0(k)=\hat{X}(k-1) \\ S_k^0=S_{k,k-1} \end{cases}$，$\hat{X}^j=\hat{X}^{j-1}(k)+K_k^j(Z_j(k)-h_j^k\hat{X}^{j-1}(j=1,2,\cdots,l)$</td>
</tr>
<tr>
<td></td>
<td>增益矩阵</td>
<td>$a_k=(h_k^1S_k^{j-1})，b_k=[(a_k)^Ta_k+D_\Delta^j(k)]，K_k^{j}=b_k^{-1}S_k^{j-1}a_k^T$</td>
</tr>
<tr>
<td></td>
<td>平方根</td>
<td>$S_k^j=S_k^{j-1}-\frac{(b_k+\sqrt(D_\Delta^j(k)b_k)}{a_k^Ta_k}K_k^ja_k^T$</td>
</tr>
<tr>
<td></td>
<td>滤波方差</td>
<td>$D_{\hat{X}(k)}=S_kS_k^T,S_k=S_k^l$</td>
</tr>
</tbody>
</table>
</div>
<p>矩阵分解地方法有多种，除了可以对方差矩阵$D_{\hat{X}}(k,k-1)$和$D_{\hat{X}}(k)$进行平方根分解外，还可以对其进行$UDU^T$分解来传递方差。</p>
<h2 id="扩展的卡尔曼滤波"><a href="#扩展的卡尔曼滤波" class="headerlink" title="扩展的卡尔曼滤波"></a>扩展的卡尔曼滤波</h2><p>对线性模型而言，卡尔曼滤波估计是最优估计，但严格地说现实中的系统并非都是线性的，由于对非线性模型的估计难度大大增加，所以通常将非线性模型线性化后进行卡尔曼滤波估计，所以这样得到的估计实际上是一种近似估计方法。这里介绍的扩展的卡尔曼滤波$EKF$将非线性模型中的状态方程在$\hat{X}(k-1)$处用泰勒公式展开并舍去二阶项和高阶项进行线性化；将观测方程在$\hat{X}(k,k-1)$处用泰勒公式展开并舍去二阶项和高阶项进行线性化，然后用线性系统的卡尔曼滤波公式进行估计。</p>
<p>非线性离散时间系统</p>
<script type="math/tex; mode=display">
X(k)=f(X(k-1),e(k-1))\\Z(k)=h(X(k))+\Delta(k)</script><p>随机模型</p>
<script type="math/tex; mode=display">
E[e(k)]=0，E[\Delta(k)]=0,\\cov[e(k)，e(j)]=D_e(k)\delta(k-j)\\cov[ \Delta(k)，\Delta(j)]=D_e(k)\delta(k-j)\\cov[e(k),\Delta(j)]=0</script><p>初始状态为</p>
<script type="math/tex; mode=display">
E[X(0)]=\mu_X(0)，var[X(0)]=D_X(0)</script><p>现在将状态方程在近似值$X_{(0)}(k-1)$和$e(k-1)=0$处用泰勒公式展开并舍去高阶项</p>
<script type="math/tex; mode=display">
X(k)=f(X_{(0)}(k-1),0)+\frac{\partial f}{\partial X(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0}(X(k-1))-X_{0}(k-1)\\+\frac{\partial f}{\partial e(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0}(e(k-1))</script><p>令</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial X(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0})=\Phi_{k,k-1}</script><script type="math/tex; mode=display">
\frac{\partial f}{\partial e(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0})=\Gamma(k-1)</script><script type="math/tex; mode=display">
\eta(k-1)=\Gamma(k-1)e(k-1)</script><p>有线性化后的状态方程</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+[f(X_{(o)}(k-1),0)-\Phi_{k,k-1}X_{(o)}(k-1)]+\eta(k-1)</script><p>将观测方程在近似值$X_{(0)}(k)$处用泰勒公式展开并舍去高阶项有</p>
<script type="math/tex; mode=display">
Z(k)=\frac{\partial f}{\partial X(k)}\mid_{X(k)=X_{(0)}(k)}(X_k-X_{(0)}(k))+h(X_{(0)}(k))+\Delta(k)</script><p>设</p>
<script type="math/tex; mode=display">
H(k)=\frac{\partial f}{\partial X(k)}\mid_{X(k)=X_{(0)}(k)}</script><p>则有线性化后的观测方程</p>
<script type="math/tex; mode=display">
Z(k)=H(k)X_k[h(X_{(0)}(k))-H(k)X_{(0)}(k)]+\Delta(k)</script><p>这时的随机模型为</p>
<script type="math/tex; mode=display">
E[\eta(k)]=0,E[\Delta(k)]=0\\cov[\eta(k),\eta(j)]=\Gamma(k)D_e(k)\Gamma^T(k)\delta(k-j)\\cov[\Delta(k),\Delta(j)]=\Gamma(k)D_{\Delta}(k)\delta(k-j)\\cov[\eta(k),\Delta(j)=0]</script><p>以上线性化模型的卡尔曼滤波为</p>
<p>1）一步预测</p>
<script type="math/tex; mode=display">
\hat{X}(k)=\Phi_{k,k-1}\hat{X}(k-1)+f(X_{(o)}(k-1),0)-\Phi_{k,k-1}X_{(o)}(k-1)</script><p>若取$X_{(0)}(k-1)=\hat{X}(k-1)$并代入上式得到</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=f(\hat{X}(k-1),0)</script><p>其方差为</p>
<script type="math/tex; mode=display">
D_{\hat{X}}(k,k-1)=\Phi_{k,k-1}D_{\hat{X}(k-1)}\Phi_{k,k-1}^T+\Gamma(k-1)D_{e(k-1)}\Gamma(k-1)^T</script><p>预测残差为</p>
<script type="math/tex; mode=display">
V_z(k)=Z(k)-[H(k)\hat{X}(k,k-1)+[h(X_{(0)}(k))-H(k)X_{(0)}(k)]]</script><p>若取$X_{(0)}(k)=\hat{X}(k,k-1)$并代入上式得到</p>
<script type="math/tex; mode=display">
V_z(k)=Z(k)-h(\hat{X}(k,k-1))</script><p>测量更新</p>
<script type="math/tex; mode=display">
\hat{X}(k)=X(k,k-1)+K_kV_z(k)\\D_{\hat{X}}(k)=(I-K_kH_k)D_{\hat{X}}(k,k-1)</script><p>增益矩阵</p>
<script type="math/tex; mode=display">
K_k=D_{\hat{X}}(k,k-1)H_k^T[H_kD_{\hat{X}}H_k^T+D_{\Delta}(k)]^{-1}</script><p>扩展的卡尔曼滤波将非线性的状态方程在$\hat{X}(k-1)$处进行线性化，将观测方程在$\hat{X}(k,k-1)$处线性化，用线性卡尔曼滤波进行估计，虽然从估计公式上看体现出了非线性函数，但实质仍然是线性估计。</p>
]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘三</title>
    <url>/2020/03/18/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%B8%89/</url>
    <content><![CDATA[<h1 id="最优估计实践（线性卡尔曼滤波）"><a href="#最优估计实践（线性卡尔曼滤波）" class="headerlink" title="最优估计实践（线性卡尔曼滤波）"></a>最优估计实践（线性卡尔曼滤波）</h1><p>卡尔曼滤波采用状态空间来描述系统，在经典估计理论中加入了状态方程，通过对被提取信号有关的量测来估计所需要的信号（状态变量）。在估计中利用的信息有：状态方程、观测方程、系统噪声、量测噪声和初始状态的统计特性，即已知系统和量测的数学模型，实时获得系统状态变量的最优估计。由于状态可以是多维，卡尔曼滤波的维数不再局限于一维，而且卡尔曼滤波算法采用递推形式，所以也可以处理非平稳的随机过程。卡尔曼滤波是一种线性，无偏且方差最小的最优估计方法。对于计算机运算来说，卡尔曼滤波的递推使其运算量和存储量大为减少，容易满足实时估计的要求。<br><a id="more"></a></p>
<h2 id="线性离散系统的卡尔曼滤波"><a href="#线性离散系统的卡尔曼滤波" class="headerlink" title="线性离散系统的卡尔曼滤波"></a>线性离散系统的卡尔曼滤波</h2><p>离散线性系统的函数模型为</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)\\Z(k)=H_kX(k)+\Delta(k)</script><p>随机模型为</p>
<script type="math/tex; mode=display">
\begin{cases}E[e(k)]=0,cov[e(k),e(j)]=D_e(k)\delta(k-j)\\E[\Delta(k)]=0,cov[\Delta(k),\Delta(j)]=D_\Delta(k)\delta(k-j)\\cov[e(k),\Delta(j)]=0 \end{cases}</script><p>式中，$\Phi_{k,k-1}$为$t_{k-1}$时刻至$t_k$时刻的转移矩阵；$\Gamma_{k,k-1}$为系统噪声驱动阵；$e(k-1)$为系统激励噪声序列；$H_k$为量测矩阵；$\Delta(k)$为量测噪声序列。</p>
<p>初始状态$X_0$的统计特性为</p>
<script type="math/tex; mode=display">
E(X(0))=\hat{X}(0)=\mu_x(0),var(X(0))=D_{\hat{X}}(0)\\cov[X(0),e(k)]=0,cov[X(0),\Delta(k)]=0</script><p>下面给出对状态$X(k)$的卡尔曼滤波估计及其推导过程。</p>
<h3 id="基于最小方差准则的推导"><a href="#基于最小方差准则的推导" class="headerlink" title="基于最小方差准则的推导"></a>基于最小方差准则的推导</h3><p>1）推导过程</p>
<p>滤波的一步预测</p>
<p>与最小二乘估计的数学模型不同的是动态系统的数学描述中增加了状态方程</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)</script><p>状态方程建立了$X(k)$与$X(k-1)$的关系，因此我们可以在得到观测值$Z(k)$前就得到$X(k)$的一些先验信息。由最小方差估计可以根据$k-1$个量测$Z(1),Z(2),\cdots,Z(k-1)$对$X(k)$做最小方差估计，记为$\hat{X}(k,k-1)$</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=E[X(k)/Z(1)Z(2)\cdots Z(k-1)] \\=E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)]</script><p>上式的条件期望也可表示为</p>
<script type="math/tex; mode=display">
E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)] \\=\iint(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))f([x(k-1),e(k-1)/z(1)z(2)\cdots  \\z(k-1))dx(k-1)de(k-1)])</script><p>因为知道$e(k-1)$只影响$X(k)$，所以$e(k-1)$与$X(k-1)$以及$Z(1),Z(2),\cdots,Z(k-1)$都不相关，所以</p>
<script type="math/tex; mode=display">
f([x(k-1),e(k-1)]/z(1)z(2)\cdots z(k-1)) \\=f(e(k-1))f(x(k-1))/z(1)z(2)\cdots z(k-1))</script><p>此式代入上一式可得</p>
<script type="math/tex; mode=display">
E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)] \\=\Phi_{k,k-1}E(X(k-1)/Z(1)Z(2)\cdots Z(k-1))</script><p>所以$\hat{X}(k,k-1)$为（$E(e(k-1)=0)$）</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=\Phi_{k,k-1}E(X(k-1))/Z(1)Z(2)\cdots Z(k-1)</script><p>由最小方差估计知道，上式中的$E(X(k-1)/Z(1)Z(2)\cdots Z(k-1))$是对$X(k-1)$的最小方差估计，即为$\hat{X}(k-1)$，即有</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)</script><p>上式表明$Z(1)Z(2)\cdots Z(k-1)$对$X(k)$的最小方差估计$\hat{X}(k,k-1)$为状态转移矩阵$\Phi_{k,k-1}$乘以最小方差估计$\hat{X}(k-1)$。$\hat{X}(k,k-1)$为未有观测值$Z(k)$前对$X(k)$的预测，称为$X(k)$的一步预测，其预测误差为</p>
<script type="math/tex; mode=display">
\Delta\hat{X}(k,k-1)=X(k)-\hat{X}(k,k-1) \\=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)-\Phi_{k,k-1}\hat{X}(k-1) \\=\Phi_{k,k-1} \Delta\hat{X}(k-1)+\Gamma_{k,k-1}e(k-1)</script><p>其中</p>
<script type="math/tex; mode=display">
\Delta\hat{X}(k-1)=X(k-1)-\hat{X}(k-1)</script><p>$\Delta\hat{X}(k,k-1)$的期望为</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=\Phi_{k,k-1} E(\Delta\hat{X}(k-1))+\Gamma_{k,k-1}E(e(k-1))</script><p> 因为$E(e(k-1))=0$，$E(\Delta\hat{X}(k-1))$=0（后面证明$\hat{X}(k-1)$是$X(k-1)$的无偏估计），所以有</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=0</script><p>即</p>
<script type="math/tex; mode=display">
E(\hat{X}(k,k-1))=E(X(k))</script><p>即证明得到 $\hat{X}(k,k-1)$ 是$X(k)$的无偏估计。</p>
<p>滤波的测量更新</p>
<p>将一步预测$\hat{X}(k,k-1)$代入观测方程，忽略观测误差可以得到观测值$Z(k)$的预测值</p>
<script type="math/tex; mode=display">
\hat{Z}(k,k-1)=H_k\hat{X}(k,k-1)</script><p>预测值与实际观测值的差异为</p>
<script type="math/tex; mode=display">
V_z(k,k-1)=Z(k)-\hat{Z}(k,k-1) \\=H_kX(k)+\Delta(k)-H_k\hat{X}(k,k-1) \\=H_k\Delta\hat{X}(k,k-1)+\Delta(k)</script><p>$V_z(k,k-1)$在滤波理论中称为预测残差（也称新息）。从上式可以看出，预测残差包含一步预测误差和观测误差。$V_z(k,k-1)$的期望为</p>
<script type="math/tex; mode=display">
E(V_z(k,k-1))=H_kE(\Delta\hat{X}(k,k-1))+E(\Delta(k))</script><p>因为$E(e(k-1))$为零，$\Delta\hat{X}(k,k-1)$的期望也为零，所以</p>
<script type="math/tex; mode=display">
E(V_z(k,k-1))=0</script><p>若将$V_z(k,k-1)$看作观测值，那么式子</p>
<script type="math/tex; mode=display">
V_z(k,k-1)=H_k\Delta\hat{X}(k,k-1)+\Delta(k)</script><p>就是将$\Delta\hat{X}(k,k-1)$视为未知参数的新的观测方程，其中</p>
<script type="math/tex; mode=display">
E[\Delta(k)]=0,cov[\Delta(k)]=D_\Delta(k)\\E(\Delta\hat{X}(k,k-1))=0,cov(\Delta\hat{X}(k,k-1))=D_{\hat{X}(k,k-1)}</script><p>根据线性最小方差估计理论可以直接得到$\Delta\hat{X}(k,k-1)$的线性最小方差估计，记为$\Delta\hat{\hat{X}}(k,k-1)$，其值为</p>
<script type="math/tex; mode=display">
\Delta\hat{\hat{X}}(k,k-1)=E(\Delta\hat{X}(k,k-1)) \\+D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}(V_z(k,k-1)-H_kE(\Delta\hat{X}(k,k-1)))</script><p>由于</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=0</script><p>有</p>
<script type="math/tex; mode=display">
\Delta\hat{\hat{X}}(k,k-1)=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}V_z(k,k-1)</script><p>又因为</p>
<script type="math/tex; mode=display">
X(k)=\Delta\hat{X}(k,k-1)+\hat{X}(k,k-1)</script><p>所以用$\Delta\hat{X}(k-1)$的线性最小方差估计值$\Delta\hat{\hat{X}}(k,k-1)$替代上式中的$\Delta\hat{X}(k-1)$可以得到由$V_z(k,k-1)$对$X(k)$的估计$\hat{X}(k)$ </p>
<script type="math/tex; mode=display">
\hat{X}(k)=\Delta\hat{X}(k,k-1)+\hat{X}(k,k-1) \\=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}V_z(k,k-1)+\hat{X}(k,k-1)</script><p>由于$V_z(k,k-1)$是由观测值$z(k)$得到，具有观测值的信息，所以上式即为观测值对$\hat{X}(k,k-1)$ 的更新，记</p>
<script type="math/tex; mode=display">
K_k=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}</script><p>有</p>
<script type="math/tex; mode=display">
\hat{X}(k)=K_kV_z(k,k-1)+\hat{X}(k,k-1)\\=\hat{X}(k,k-1)+K_k(Z(k)-H_k\hat{X}(k,k-1))</script><p>至此，我们得到了对$X(k)$的一步预测和测量更新估计的递推公式。</p>
<script type="math/tex; mode=display">
\begin{cases}\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)\\ \hat{X}(k)=\hat{X}(k,k-1)+K_k(Z(k)-H_k\hat{X}(k,k-1))\end{cases}</script><p>$\hat{X}(k)$的期望为</p>
<script type="math/tex; mode=display">
E(\hat{X}(k))=K_kE(V_z(k,k-1))+E(\hat{X}(k,k-1))=E(\hat{X}(k,k-1))=E(X(k))</script><p>即$X(k)$的观测值更新为无偏估计。</p>
<p>以上就是根据最小方差准则得到的卡尔曼滤波器。卡尔曼滤波器还可以用正交投影法和递推最小二乘准则推导，正交投影推导可以使我们看到卡尔曼滤波器的数学内涵：$\hat{X}(k)$是$X(k)$在$Z(k)$上的正交投影，在递推最小二乘的推导中，将一步预测的$\hat{X}(k,k-1)$看作虚拟观测值，与$Z(k)$一并组成观测方程进行最小二乘平差，最后得到的$X(k)$与最小方差估计$\hat{X}(k)$相等，这也说明卡尔曼滤波估计是残差平方和最小的估计。</p>
<h3 id="滤波器稳定性判别"><a href="#滤波器稳定性判别" class="headerlink" title="滤波器稳定性判别"></a>滤波器稳定性判别</h3><p>由于滤波方程由系统的状态方程和观测方程给出，滤波的稳定性应该与随机线性系统的结构和参数有关。如果线性系统是随机一致完全可控和随机一致完全可测的，那么卡尔曼滤波器是一致渐进稳定的。</p>
<p>离散时不变系统一致完全随机可控的条件为</p>
<script type="math/tex; mode=display">
rank[\Gamma \quad \Phi\Gamma  \quad  \cdots\quad \Phi^{N-1}\Gamma]=n</script><p>离散时不变系统一致完全随机可测的条件为</p>
<script type="math/tex; mode=display">
rank[H \quad H\Phi  \quad  \cdots\quad H\Phi^{N-1}]^T=n</script>]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘二</title>
    <url>/2020/03/18/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%BA%8C/</url>
    <content><![CDATA[<h1 id="动态系统的数学模型及性质"><a href="#动态系统的数学模型及性质" class="headerlink" title="动态系统的数学模型及性质"></a>动态系统的数学模型及性质</h1><p>备忘一介绍了参数估计的数学模型，它描述观测值与参数之间的关系，是静态估计模型。静态估计模型不考虑研究对象的状态在时间上的变化，但生活中大量的研究对象是一个动态系统，在估计时我们必须考虑研究对象的运动变化规律，所以在对动态系统的估计中，除了观测方程以外，还需要对动态系统的物理变化规律或者动力学状态进行数学描述，即描述系统的“输入”与系统“状态”之间的关系，所以称为状态方程。为了研究系统的特性我们对系统进行观测，观测量即为系统的输出，所以观测方程描述了输出与状态之间的关系。状态方程和观测方程构成了动态系统的数学模型。</p>
<a id="more"></a>
<h2 id="状态方程及其对应的解"><a href="#状态方程及其对应的解" class="headerlink" title="状态方程及其对应的解"></a>状态方程及其对应的解</h2><p>设$X(t)$为状态变量，$A(t)$、$B(t)$ 为系数矩阵，$u(t)$ 为输入矩阵，$X(t_0)=X_0$为系统初值</p>
<p>1)若$A(t)$不随时间变化，即为常数矩阵$A$，则有线性时不变齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)</script><p>根据常微分方程的解法并由初始条件可以得到</p>
<script type="math/tex; mode=display">
X(t)=e^{A·(t-t_0)}X(t_0)=e^{A·(t-t_0)}X_0</script><p>现设</p>
<script type="math/tex; mode=display">
\Phi(t-t_0)=e^{A·(t-t_0)}</script><p>则有解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)</script><p>上式说明齐次状态方程的解实质是初始状态$X(t_0)$从初始时刻到时刻$t$系统运动状态的转移，所以$\Phi(t-t_0)$称为状态转移矩阵。</p>
<p>2)连续时不变系统（连续定常系统，$A$、$B$为常数矩阵）非齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+\mathop{B}\mathop{u}(t)</script><p>解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)+\int_{t_0}^te^{A(t-\tau)}·Bu(\tau)d\tau</script><p>上式表明系统在任何时刻的状态取决于系统的初始状态和从初始时刻以后的输入。</p>
<p>3)若动态系统无输入或者不考虑系统的输入，无输入的状态方程为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}(t)\mathop{X}(t)</script><p>上式表明系统本身在无外力的作用下自由运动，这样的状态方程称为齐次微分方程。</p>
<p>设上式的解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t,t_0)X(t_0)</script><p>（注意这时的转移矩阵不是$\Phi(t-t_0)$ ，代入状态方程有  </p>
<script type="math/tex; mode=display">
\mathop{\Phi}\limits^{·}(t,t_0)X(t_0)=A(t)\Phi(t,t_0)X(t_0)</script><p>因此有</p>
<script type="math/tex; mode=display">
\begin{cases}\mathop{\Phi}\limits^{·}(t,t_0)=A(t)\Phi(t,t_0)\\\Phi(t,t_0)=I \end{cases}</script><p>求得满足上述条件的$\Phi(t,t_0)$，$X(t)=\Phi(t,t_0)X(t_0)$即为状态方程的解。</p>
<p>4）连续时变系统非齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}\limits_{n×1}’(t)=\mathop{A}\limits_{n×n}(t)\mathop{X}\limits_{n×1}(t)+\mathop{B}\limits_{n×p}(t)\mathop{u}\limits_{p×1}(t)</script><p>解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)+\int_{t_0}^t\Phi(t,\tau)·B(\tau)u(\tau)d\tau</script><h2 id="连续线性动态系统的函数模型与随机模型"><a href="#连续线性动态系统的函数模型与随机模型" class="headerlink" title="连续线性动态系统的函数模型与随机模型"></a>连续线性动态系统的函数模型与随机模型</h2><p>在对动态系统进行描述时，总有一些未知的不确定的因素，故引入系统噪声$e(t)$（系统误差）。为研究系统的运动规律，需要对系统进行观测，其观测量设为$Z(t)$，观测噪声设为$\Delta(t)$（观测误差）。</p>
<p>1）当系统为时不变系统时，函数模型为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+\mathop{B}\mathop{u}(t)+Ce(t)\\Z(t)=HX(t)+Gu(t)+\Delta(t)</script><p>2）当动态系统没有控制输入或者不考虑系统的输入时，函数模型为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}(t)\mathop{X}(t)+Ce(t)\\Z(t)=H(t)X(t)+\Delta(t)</script><p>3)当系统为时不变且没有控制输入时</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+Ce(t)\\Z(t)=HX(t)+\Delta(t)</script><p>在解决现实问题时，通常假设系统噪声和观测噪声为零均值白噪声，且它们之间完全不相关。</p>
<h2 id="离散线性动态系统的函数模型与随机模型"><a href="#离散线性动态系统的函数模型与随机模型" class="headerlink" title="离散线性动态系统的函数模型与随机模型"></a>离散线性动态系统的函数模型与随机模型</h2><p>对动态系统的观测是基于某些离散的时间点上的观测，所以在对动态系统的估计需要将其状态方程离散化得到离散模型从而便于计算机处理数据。对连续动态系统离散化可以采用两种方法：1）是求得常微分方程得解析解，也就是求得状态转移矩阵，通过状态在时间上的转移从而得到各个离散时刻的系统状态值。2）是根据动态方程的数值解直接递推得到$X(t_k)$时刻的近似值。</p>
<h2 id="动态系统可控性和可测性"><a href="#动态系统可控性和可测性" class="headerlink" title="动态系统可控性和可测性"></a>动态系统可控性和可测性</h2><p>动态系统数学模型中的状态方程描述了控制输入量及初始状态对系统内部状态的影响，表明了系统内部结构特性，但不是所有的状态方程中的状态变量都受输入量的控制同时也不是所有系统的状态变量都能被观测到，由此引出了动态系统可控和可测性的问题。</p>
<p>1）可控性</p>
<p>在时间区间$[t_0,t_1]$，如果控制系统的输入$u(t)(t\in[t_0,t_1],t_1&gt;t_0)$可以将初始值$X(t_0)$转移到$X(t_1))$，$(X(t_1)$</p>
<p>为任意值），即系统的每一个状态变量都是可以控制的，这样的系统是完全可控的。</p>
<p>连续时不变动态系统的可控性条件为</p>
<script type="math/tex; mode=display">
rank[\mathop{B}\limits_{n×p} \quad \vdots\quad \mathop{AB}\limits_{n×p}\quad \vdots\quad \mathop{A^2B}\limits_{n×p}\quad \vdots\quad \cdots\quad \vdots\quad \mathop{A^{n-1}B}\limits_{n×p}\quad ]=n</script><p>离散线性时不变系统的可控性条件为</p>
<script type="math/tex; mode=display">
rank[\mathop{\Phi^0\Psi}\limits_{n×p} \quad \vdots\quad \mathop{\Phi\Psi}\limits_{n×p}\quad \vdots\quad \mathop{\Phi^2\Psi}\limits_{n×p}\quad \vdots\quad \cdots\quad \vdots\quad \mathop{\Phi^{k-1}\Psi}\limits_{n×p}\quad ]=n</script><p>2)可测性</p>
<p>可测性指能否从系统的输出中观测到系统内部信息的特性。在时间区间$[t_0,t_1]$，根据$t_0$到$t_1$的观测值$Z(t)(t\in[t_0,t_1],t_1&gt;t_0)$可以唯一地确定系统在初始时刻的状态$X(t_0)$,，则称系统是完全可测的。</p>
<p>连续时不变动态系统的可测性条件为</p>
<script type="math/tex; mode=display">
rank\begin{bmatrix}\mathop{H}\limits_{l×n}\\ \mathop{HA^1}\limits_{l×n} \\ \mathop{\vdots} \\ \mathop{H}\limits_{l×n}\mathop{A^{n-1}}\limits_{n×n} \end{bmatrix}=n</script><p>离散线性时不变系统的可测性条件为</p>
<script type="math/tex; mode=display">
rank\begin{bmatrix}\mathop{H}\limits_{l×n}\\ \mathop{H\Phi^1}\limits_{l×n} \\ \mathop{\vdots} \\ \mathop{H}\mathop{\Phi^{n-1}}\limits_{l×n} \end{bmatrix}=n</script>]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘一</title>
    <url>/2020/03/12/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%B8%80/</url>
    <content><![CDATA[<h1 id="基础理论篇"><a href="#基础理论篇" class="headerlink" title="基础理论篇"></a>基础理论篇</h1><h2 id="最小二乘估计"><a href="#最小二乘估计" class="headerlink" title="最小二乘估计"></a>最小二乘估计</h2><p>1）最小二乘估计方法由德国数学家高斯提出，估计准则为残差（测绘中称为改正数）的平方和达到最小。假定有一个数学模型为</p>
<script type="math/tex; mode=display">
Z_{l×1}=H_{l×n}X_{n×1}+\nabla_{l×1} \quad (1.1)</script><script type="math/tex; mode=display">
E(Z)=HX \quad var(Z)=D \quad (1.2)</script><p>若现在通过某种方法得到参数估计$\hat{X}$，代入观测方程后可得到估计的观测值：<br><a id="more"></a></p>
<script type="math/tex; mode=display">
\hat{Z}_{l×1}=H_{l×n}\hat{X}_{n×1} \quad (1.3)</script><p>那么估计的观测值与实际观测值的差异为</p>
<script type="math/tex; mode=display">
v=\hat{Z}-Z=H\hat{X}-Z \quad (1.4)</script><p>其中</p>
<script type="math/tex; mode=display">
v=[v_1\quad v_2 \quad v_3 \quad \cdots \quad v_l]^T\quad (1.5)</script><p>上式中$v_i$表示观测值$Z_i$的残差，$v$也称为观测值的残差向量。要使观测值的残差平方和最小，即</p>
<script type="math/tex; mode=display">
\sum\limits_{i=1}^{i=l}v_i^2=min\quad (1.6)</script><p>向量的形式为</p>
<script type="math/tex; mode=display">
v^Tv=min\quad (1.7)</script><p>在方程（1.4）的无穷多组解中，能满足（1.6）的解即为最小二乘解$\hat{X}_{LS}$。</p>
<p>式1.6的准则认为所有观测值$Z_1,Z_2,\cdots,Z_l$对参数的影响相同，但有的时候观测值是不同精度的，所以在估计时我们希望精度好的观测值对参数估计的影响大，即方差小的观测值对参数估计的影响大，反之方差大的对参数估计的影响小，所以在式1.6的基础上根据观测值的方差赋予观测值一定的权重。权矩阵可设为</p>
<script type="math/tex; mode=display">
W=\sigma_0^2D^{-1}\quad (1.8)</script><p>，可证明$\sigma_0^2$的取值不影响最小二乘参数估计值，所以$\sigma_0^2$可以取任意实常数。亦可证明若观测值相互独立，$W$为对角矩阵</p>
<script type="math/tex; mode=display">
W=\begin{bmatrix}W_1& \quad & \quad & \quad \\
 \quad &W_2 &\quad  &\quad \\
\quad &\quad &\cdots &\quad \\
\quad &\quad &\quad &W_l \end{bmatrix}=\begin{bmatrix}\frac{\sigma_0^2}{\sigma_1^2}& \quad & \quad & \quad \\
 \quad &\frac{\sigma_0^2}{\sigma_2^2} &\quad  &\quad \\
\quad &\quad &\frac{\sigma_0^2}{\sigma_3^2} &\quad \\
\quad &\quad &\quad&\frac{\sigma_0^2}{\sigma_4^2} \end{bmatrix}\quad (1.9)</script><p>可以看到观测值的权与其方差成反比。</p>
<p>$W$为对角矩阵时最小二乘准则为</p>
<script type="math/tex; mode=display">
\sum\limits_{i=1}^{i=l}v_i^2W_i=min\quad (1.10)</script><p>最小二乘准则更一般的形式为</p>
<script type="math/tex; mode=display">
v^TWv=min\quad (1.11)</script><p>现建立满足最小二乘准则的目标方程</p>
<script type="math/tex; mode=display">
\Phi(\hat{X})=v^TWv=min\quad (1.12)</script><p>将误差方程1.4代入目标函数1.12</p>
<script type="math/tex; mode=display">
\Phi(\hat{X})=（H\hat{X}-Z）^TW（H\hat{X}-Z）=\hat{X}^TH^TWH\hat{X}-\hat{X}^TH^TWZ-Z^TWH\hat{X}+Z^TWZ\quad (1.13)</script><p>$\Phi(\hat{X})$是$\hat{X}$的函数，为了使其最小，由求函数极值方法（此方程可看成开口向上的二次函数，有极小值）得到</p>
<script type="math/tex; mode=display">
\frac{\partial\Phi(\hat{X})}{\partial\hat{X}}=2H^TWH\hat{X}-2H^TWZ=0\quad (1.14)</script><p>由于$H$为列满秩矩阵，$W$为对称方阵，所以$H^TWH$为满秩矩阵（可逆），方程有解且唯一，解得</p>
<script type="math/tex; mode=display">
\hat{X}_{LS}=(H^TWH)^{-1}H^TWZ\quad (1.15)</script><p>$\hat{X}_{LS}$即为满足最小二乘估计准则的解，式1.8代入式1.15得</p>
<script type="math/tex; mode=display">
\hat{X}_{LS}=(H^T\sigma_0^2D^{-1}H)^{-1}H^T\sigma_0^2D^{-1}Z=(H^TD^{-1}H)^{-1}H^TD^{-1}Z\quad (1.16)</script><p>由此证明$\sigma_0^2$的数值不影响$\hat{X}_{LS}$，$\hat{X}_{LS}$的值由设计矩阵$H$、观测值$Z$和观测值的方差矩阵$D$决定。</p>
<p>易证最小二乘估计为无偏估计且残差$v$与$\hat{X}_{LS}$不相关。</p>
<p>2）附有约束条件的最小二乘估计</p>
<p>线性化后的附有参数约束条件的函数模型为</p>
<script type="math/tex; mode=display">
\mathop{z}\limits_{l×1}=\mathop{H}\limits_{l×n}\mathop{x}\limits_{n×1}+\mathop{\Delta}\limits_{l×1}\\\mathop{C}\limits_{c×n}\mathop{x}\limits_{n×1}+\mathop{\varphi}\limits_{c×1}(X_0)=0</script><p>$H$为列满秩矩阵，$C$为行满秩矩阵，其秩为参数的约束条件的个数。</p>
<p>随机模型为</p>
<script type="math/tex; mode=display">
E(z)=HX \quad var(z)=D</script><p>现假设估计得到的$x$为$\hat{x}_{LS}$，那么误差修正后的观测值</p>
<script type="math/tex; mode=display">
\hat{z}=H\hat{x}_{LS}</script><p>观测值的残差为</p>
<script type="math/tex; mode=display">
v=H\hat{x}_{LS}-z</script><p>$\hat{x}_{LS}$同时满足约束条件</p>
<script type="math/tex; mode=display">
C\hat{x}_{LS}+\varphi(X_0)=0</script><p>联立上两式得到</p>
<script type="math/tex; mode=display">
\begin{bmatrix}I&-H\\0&C\end{bmatrix}\begin{bmatrix}v\\\hat{x}_{LS}\end{bmatrix}=\begin{bmatrix}-Z\\-\varphi(X_0)\end{bmatrix}</script><p>依题可知上式是有无穷组解的相容方程，现在要在这无穷多组解中找到能够满足最小二乘准则$v^TWv=min$并满足约束条件的一组解，按拉格朗日条件极值法构造方程</p>
<script type="math/tex; mode=display">
\Phi(\hat{x})=v^TWv+2K^T(Cx+\varphi(X_0))</script><p>约束条件代入上式可得</p>
<script type="math/tex; mode=display">
\Phi(\hat{x})=\hat{x}_{LS}^TH^TWH\hat{x}_{LS}-\hat{x}_{LS}^TH^TWz-z^TWH\hat{x}_{LS}+z^TWz+2K^T(Cx+\varphi(X_0))</script><p>上式对$x$求导并令其为零可得</p>
<script type="math/tex; mode=display">
H^TWH\hat{x}_{LS}-H^TWz+C^TK=0</script><p>与约束条件联立有</p>
<script type="math/tex; mode=display">
\begin{bmatrix}H^TWH&C^T\\C&0\end{bmatrix}\begin{bmatrix}\hat{x}_{LS}\\K\end{bmatrix}=\begin{bmatrix}H^TWz\\-\varphi(X_0)\end{bmatrix}</script><p>解得附有约束条件的最小二乘估计 $\hat{x}_{LS}$</p>
<script type="math/tex; mode=display">
\begin{cases}K=(C(H^TWH)^{-1}C^T)^{-1}(C(H^TWH)^{-1}HWz+\varphi(X_0))\\\hat{x}_{LS}=((H^TWH)^{-1}-(H^TWH)^{-1}C^T(C(H^TWH)^{-1}C^T)^{-1}C(H^TWH)^{-1})HWz\\-(H^TWH)^{-1}C^TC(H^TWH)^{-1}C^T)^{-1}\varphi(X_0)\end{cases}</script><p>3）递推最小二乘估计</p>
<p>上面介绍得两种最小二乘估计方法是集中所有观测值对参数进行估计得方法，即批处理方法。该方法会占用计算机的大量内存，不能实时对数据进行处理，解决这个问题的方法是采用最小二乘的递推算法。</p>
<p>设在第$k$次观测后，观测值$z_k$对应的误差方程为</p>
<script type="math/tex; mode=display">
V_k=H_k\hat{X}-Z_k</script><p>观测值向量$Z_k$的方差阵为$D_k$，权矩阵为$W_k=\sigma_0^2D_k$，因此这时参数的最小二乘解为</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k)}=(H_k^TW_kH_k)^{-1}H_k^TW_kZ_k</script><p>$k+1$次观测值向量为$z_{k+1}$，对应的误差方程为</p>
<script type="math/tex; mode=display">
v_{k+1}=h_{k+1}\hat{X}-z_{k+1}</script><p>观测值$z_{k+1}$的方差阵为$d_{k+1}$，权矩阵为$w_{k+1}=\sigma_0^2d_{k+1}^{-1}$。现集中所有的观测值，数量为$l_{k+1}$，构成观测值向量$Z_{k+1}=[Z_k \quad z_{k+1}]^T$，其对应的误差方程为</p>
<script type="math/tex; mode=display">
V_{k+1}=H_{k+1}\hat{X}-Z_{k+1}</script><p>其中</p>
<script type="math/tex; mode=display">
V_{k+1}=\begin{bmatrix}V_k\\v_{k+1}\end{bmatrix} \quad H_{k+1}=\begin{bmatrix}H_k\\h_{k+1}\end{bmatrix} \quad Z_{k+1}=\begin{bmatrix}Z_k\\z_{k+1}\end{bmatrix}</script><p>观测值向量$Z_{k+1}$对应的权矩阵为</p>
<script type="math/tex; mode=display">
\quad W_{k+1}=\begin{bmatrix}W_k& \quad\\ \quad&w_{k+1}\end{bmatrix}</script><p>以上是批处理算法的数学模型，其参数估计为</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k+1)}=(H_{k+1}^TW_{k+1}H_{k+1})^{-1}H_{k+1}^TW_{k+1}Z_{k+1}\\=(H_k^TW_kH_k+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}(H_k^TW_kZ_k+h_{k+1}^Tw_{k+1}z_{k+1})</script><p>又$\hat{X}_{B(k+1)}$的协因数矩阵为</p>
<script type="math/tex; mode=display">
Q_{\hat{X}_{B(k+1)}}=(H_k^TW_kH_k+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}=(Q_{\hat{X}_{B(k)}}^{-1}+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}</script><p>代入上一式可得</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k+1)}=\hat{X}_{B(k)}+Q_{\hat{X}_{B(k+1)}}h_{k+1}^Tw_{k+1(z_{k+1}-h_{k+1}\hat{X}_{B(k)})}</script><p>上式即为递推最小二乘估计的参数估计式。</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>极大似然估计提供了一种给定观测值来评估模型参数的方法，即“模型已定，参数位置”，为了能够估计得到分布中的未知参数$x$，一个自然的想法是一组未知参数的估值$\hat{x}$决定的概率密度函数$p(z/\hat{x})$最大，这意味着这组观测值在这个参数估计下出现的”可能性“的概率最大，即</p>
<script type="math/tex; mode=display">
L(x)=p(z/\hat{x})=max</script><p>基于这个准则的方法称为极大似然估计，$L(x)$称为似然函数，它是参数$x$的条件分布。求总体参数$x$的极大似然估计值的问题就是求似然函数$L(x)$的最大值问题。对似然函数求导</p>
<script type="math/tex; mode=display">
\frac{\partial p(z/x)}{\partial x}\mid_{x=\hat{x}_{ML}} =0</script><p>等价于（对数函数单增，两式极值点一致）</p>
<script type="math/tex; mode=display">
\frac{\partial lnp(z/x)}{\partial x}\mid_{x=\hat{x}_{ML}} =0</script><p>（1）极大似然函数估计需要已知观测值的概率密度函数即通过已知观测值与未知参数的条件分布来建立似然函数，而最小二乘估计只需要知道观测值与参数的函数关系和观测值的特征值；</p>
<p>（2）极大似然估计并不考虑参数的先验分布，也就是说极大似然估计中，将$X$视为随机变量；</p>
<p>（3）极大似然估计并不是线性估计但当观测值服从正态分布并且观测值与参数之间是线性关系时，极大似然估计与最小二乘估计等价；</p>
<h2 id="极大验后估计"><a href="#极大验后估计" class="headerlink" title="极大验后估计"></a>极大验后估计</h2><p>极大似然估计是以”$L(x)=p(z/\hat{x})=max$“为估计准则，在其估计中并没有考虑参数$X$的先验信息。极大验后估计考虑到了$X$的先验信息是使验后条件概率密度函数$p(x\mid z)$最大的$\hat{x}$为参数的估计，即</p>
<script type="math/tex; mode=display">
p(x\mid z)=max</script><p>它的含义是，给定一组观测值$Z$，在这组观测值的条件下使得$x$出现的概率最大，故极大验后估计满足</p>
<script type="math/tex; mode=display">
\frac{\partial lnp(z\mid x)}{\partial x}\mid_{x=\hat{x}_{MAP}} =0</script><h2 id="最小（均）方差估计"><a href="#最小（均）方差估计" class="headerlink" title="最小（均）方差估计"></a>最小（均）方差估计</h2><p>1）均方差（MSE）反映的是估计值与其真值之间的密集程度或者估计值的真误差在零附近的密集程度，这里的方差估计是无偏估计，均方差就是方差，故称为最小方差估计。最小方差估计的准则为</p>
<script type="math/tex; mode=display">
J_0(\hat{X}(Z))=E[(\hat{X}-X)(\hat{X}-X)^T]=min</script><p>式中，$\hat{X}$是通过观测值$Z$得到的对$X$的估计，它是$Z$的函数，所以$(\hat{X}-X)(\hat{X}-X)^T$是$X$和$Z$的函数，假设$f(x,z)$为$X$和$Z$的联合分布，那么</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\iint(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dxdz</script><p>$\because$    $f(z)\geq0$恒成立，故上式等价于</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\int(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dx=min</script><p>解得</p>
<script type="math/tex; mode=display">
E(X/Z)-\hat{x}=0</script><p>即最小方差准则下的参数估计为</p>
<script type="math/tex; mode=display">
\hat{X}_{MV}=E(X/Z)</script><p>若把最小方差估计的准则改为使$J_0(\hat{X}(Z))$的迹最小，即</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}(Z)))=E[(\hat{X}-X)(\hat{X}-X)^T]=min</script><p>即</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}(Z)))=\int(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dx\\=\hat{x}^T\hat{x}+\int x^Txf(x/z)dx-2\hat{x}^T\int xf(x/z)dx=min</script><p>上式对$\hat{x}$求导并零其为零得到</p>
<script type="math/tex; mode=display">
2\hat{x}-2\int xf(x/z)=0</script><p>因为$\int xf(x/z)dx=E(X/Z)$，所以方差的迹最小的解为</p>
<script type="math/tex; mode=display">
\hat{X}_{MV}=E(X/Z)</script><p>即方差迹最小的解与方差最小的解结果完全相同，所以可以用方差迹最小的准则来代替方差最小准则来得到方差最小的解。</p>
<p>对于任何分布而言，参数$X$的最小方差估计$\hat{X}_{MV}$都为其验后期望，在正态分布的情况下，极大验后估计也为$E(X/Z)$，即极大验后估计与最小方差估计等价。</p>
<p>2）线性最小方差估计</p>
<p>线性最小方差估计是一种特殊的最小方差估计，它是指估计值$\hat{X}$是观测量$Z$的线性函数，并使得估计的均方差最小的估计，即估计量具有如下形式</p>
<script type="math/tex; mode=display">
\mathop{\hat{X}}\limits_{n×1}=\mathop{a_L}\limits_{n×1}+\mathop{B_L}\limits_{n×l}\mathop{Z}\limits_{l×1}</script><p>同时估计量</p>
<script type="math/tex; mode=display">
\hat{X}=MSE(\hat{X})=E[(X-\hat{X})(X-\hat{X})^T]=min</script><p>由上面对最小方差估计得推导得$J_0(\hat{X})=min$等价于</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}))=E[(X-a_L-B_LZ)^T(X-a_L-B_LZ)]=min</script><p>根据极值理论将$tr(J_0(\hat{X}))$对$a_L$和$B_L$分别求偏导并令其为零</p>
<script type="math/tex; mode=display">
E(X-a_L-B_LZ)=0\\E((X-a_L-B_LZ)^TZ)=0</script><p>上两式联立解得</p>
<script type="math/tex; mode=display">
\begin{cases}a_L=E(X)-D_{XZ}D_Z^{-1}E(Z)\\B_L=D_{XZ}D_Z^{-1}\end{cases}</script><p>$a_L$和$B_L$表示满足线性最小方差的解，最后将解代入估计式得到最小方差估计</p>
<script type="math/tex; mode=display">
\hat{X}_L=a_L+B_LZ=E(X)+D_{XZ}D_Z^{-1}(Z-E(Z))</script><p>线性最小方差估计与最小二乘估计一样并不需要知道随机变量具体分布，但线性最小方差估计利用了参数$X$的先验随机信息$\mu_x$（期望）和$D_x$（方差），也就是说当有参数$X$的先验随机信息时，线性最小方差估计改进了最小二乘估计。</p>
<h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>在估计某个量时，随机误差的干扰使估计产生误差</p>
<script type="math/tex; mode=display">
\nabla X=X-\hat{X}(Z)</script><p>这种差异造成的损失可以用损失函数来描述，即损失函数为</p>
<script type="math/tex; mode=display">
L(\nabla X)=L(X,\hat{X}(Z))</script><p>如何来量化着这种损失有不同的定义，一般而言，估计误差越大，损失就越大，典型的损失函数有：</p>
<p>平方损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=(X-\hat{X})(X-\hat{X})^T</script><p>绝对值损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=\mid X-\hat{X}(Z)\mid</script><p>均值损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=\begin{cases}0 \quad (\mid\nabla X\mid\leq\frac{\nabla}{2})\\ 1 \quad (\mid\nabla X\mid＞\frac{\nabla}{2})\end{cases}</script><p>损失函数在随机变量的整个域内的平均值，即期望，定义为贝叶斯风险</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=\int_{-∞}^{+∞}\int_{-∞}^{+∞}L(X,\hat{X}(Z))f(x,z)dxdz</script><p>贝叶斯估计就是得到使上述的风险值最小的估计</p>
<script type="math/tex; mode=display">
\hat{X}_B(Z)=argmin(R_B(X,\hat{X}(Z)))</script><p>即</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=min</script><p>下面根据不同的损失函数来推导贝叶斯估计</p>
<p>1）平方损失函数</p>
<p>平方损失函数的贝叶斯风险</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=\int_{-∞}^{+∞}\int_{-∞}^{+∞}(X-\hat{X})(X-\hat{X})^Tf(x,z)dxdz</script><p>将上式与</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\iint(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dxdz</script><p>比较可知平方损失函数的贝叶斯风险就是其均方差，所以贝叶斯风险最小的参数估计</p>
<script type="math/tex; mode=display">
\hat{X}_B(Z)=\hat{X}_{MV}=E(X/Z)</script><p>2)绝对损失函数</p>
<p>绝对损失函数的验后风险为</p>
<script type="math/tex; mode=display">
r_{abs}(\hat{X}(Z)/Z)=\int_{-∞}^{+∞}\mid x-\hat{x}(z) \mid f(x/z)dx\\=\int_{-∞}^{\hat{x}(z)}(\hat{x}(z)-x )f(x/z)dx+\int_{\hat{x}(z)}^{+∞}( \hat{x}(z)-x) f(x/z)dx=min</script><p>对上式求导，并令导数在$\hat{X}(z)=\hat{X}_{abs}(z)$处为零，得</p>
<script type="math/tex; mode=display">
\int_{-∞}^{\hat{x}_{abs}(z)}f(x/z)dx=\int_{\hat{x}_{abs}(z)}^{+∞}f(x/z)dx</script><p>由上式可知，贝叶斯估计为条件概率密度$f_{X/Z(x/z)}$得中位数，即$\hat{X}_{abs}(z)=\hat{X}_{med}$，所以也称为条件中位数估计，记为$\hat{X}_{med}$。</p>
<p>3）均匀损失价函数</p>
<p>均匀损失价函数验后风险为</p>
<script type="math/tex; mode=display">
r_{unf}(\hat{X}(Z)/Z)=\int_{-∞}^{\hat{X}(Z)-\frac{\Delta}{2}} f(x/z)dx+\int_{\hat{X}(Z)+\frac{\Delta}{2}}^{+∞} f(x/z)dx =1-\int_{\hat{X}(Z)-\frac{\Delta}{2}}^{\hat{X}(Z)+\frac{\Delta}{2}} f(x/z)dx=min</script><p>上式等价于</p>
<script type="math/tex; mode=display">
f_{X/Z}(x/z) \mid_{x=\hat{x}}=max</script><p>即贝叶斯估计为极大验后估计$\hat{X}_{MAP}$，由于$f_{X/Z}(x/z)$不容易得到，由条件概率公式可知上式也等价于</p>
<script type="math/tex; mode=display">
f_{Z/X}(z/x) ×f_x(x)\mid_{x=\hat{x}_{MAP}}=max</script><h2 id="各个估计方法的总结对比"><a href="#各个估计方法的总结对比" class="headerlink" title="各个估计方法的总结对比"></a>各个估计方法的总结对比</h2><p>（1）最小二乘估计不需要知道随机变量得具体分布，只需要已知观测值与参数得关系和观测值得方差，将估计参数$X$视为非随机变量，其估计是线性估计。</p>
<p>（2）极大似然估计需要知道参数条件下的观测值的具体分布$p(z/x)$，在其估计中并没有考虑$X$的先验分布，即视$X$为非随机变量。当观测值与参数呈线性关系并且正态分布时，极大似然估计与最小二乘估计等价。</p>
<p>（3）极大验后估计需要已知$p(z \mid x)$和$p(x)$。与极大似然估计比较，极大验后估计用了$X$的先验分布$p(x)$，所以改进了极大似然估计。当$p(z,x)$服从正态分布时，其估计值为$E(X/Z)$，这与最小方差估计等价。</p>
<p>（4）最小方差估计需要已知分布$p(x\mid z)$或者$E(X/Z)$，其估计值为$E(X/Z)$，与极大验后估计一样，最小方差估计利用了$X$的先验随机信息。</p>
<p>（5）线性最小方差估计需要在最小方差的准则上满足其估计与观测值呈线性关系。线性最小方差估计对已知条件要求较为宽松，只需要知道观测值$Z$和参数$X$的期望、方差、协方差。</p>
<p>（6）贝叶斯估计使估计误差造成的损失在整个验后分布的平均值最小，即损失函数的验后期望最小。不同的损失函数得到不同的估计，如当损失函数为平方损失函数时，其估计与最小方差估计等价；当损失函数为均匀损失函数时，其估计与最大验后估计等价。</p>
<p>（7）最小二乘估计和线性最小方差估计为线性估计，其它方法只有在观测值与参数呈线性关系，并且是正态分布时才是线性估计。</p>
]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/03/11/hello-world/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
