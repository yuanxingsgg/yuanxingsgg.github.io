<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>视觉里程计学习备忘</title>
    <url>/2020/03/28/%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1/</url>
    <content><![CDATA[<p>SLAM系统分为前端和后端两个部分，其中前端也称为视觉里程计，其作用是根据相邻图像的信息估计出粗略的相机运动，给后端提供较好的初始值。视觉里程计的实现算法主要有特征点法、光流法和直接法。基于特征点法的前端稳定，对光照、动态物体不敏感，但与此同时它也有以下几个缺点：<br><a id="more"></a></p>
<ol>
<li>关键点的提取与描述子的计算非常耗时。实践中，SIFT目前在CPU上是无法实时计算的，而ORB也需要将近20毫秒的计算时长。</li>
<li>使用特征点时，忽略了除特征点以外的所有信息。只使用特征点丢弃了大部分可能有用的信息。</li>
<li>相机有时候会运动到特征丢失的地方，这些地方往往没有明显的纹理信息，这种情况下可能找不到足够的匹配点来计算相机运动。</li>
</ol>
<p>为克服这些缺点，可以采用的措施有：</p>
<ol>
<li>保留特征点，不计算描述子。同时使用光流法跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间，而光流本身的计算时间要小于描述子的计算与匹配。</li>
<li>只计算关键点，不计算描述子。同时使用直接法计算特征点在下一时刻图像中的位置。这同样可以跳过描述子的计算过程，也省去了光流的计算时间。</li>
</ol>
<h1 id="特征点法"><a href="#特征点法" class="headerlink" title="特征点法"></a>特征点法</h1><p>视觉里程计的核心问题是如何根据图像估计相机运动。特征点是图像里一些特别的地方，在相机运动之后仍保持稳定。特征点由关键点和描述子两部分组成，关键点是指该特征点在图像里面的位置，有些还有朝向、大小等信息；描述子通常是一个向量，按照某种人为设计的方式，描述了该关键点周围像素的信息。描述子设计原则是“外观相似的特征应该由相似的描述子”，因此只要两个特征点的描述子在向量空间上的距离相近，就可以认为它们是同样的特征点。目前常用的特征点由SIFT、FAST、ORB等等。</p>
<p>ORB特征</p>
<p>ORB特征由关键点描述子两部分组成。它的关键点称为“Oriented FAST”，是一种改进的FAST角点，描述子称为“BRIEF”。因此提取ORB特征分为两个步骤：</p>
<ol>
<li>FAST角点提取</li>
<li>BRIEF描述子提取</li>
</ol>
<p>FAST关键点</p>
<p>FAST是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与邻域的像素差别过大（过亮或过暗），那么它更可能是角点。相比于其他角点检测算法，FAST只需比较像素亮度的大小，十分快捷。FAST特征点的计算仅仅是比较像素间亮度的差异，所以速度非常快，但它也有重复性不强、分布不均匀的缺点。此外，FAST不具有方向和尺度信息，基于此，ORB添加了尺度和旋转的描述。尺度不变性由构建图像金字塔（对图像进行不同层次的降采样，以获得不同分辨率的图像），并在金字塔的每一层上检测角点来实现，特征的旋转由灰度质心（以图像块灰度值作为权重的中心）法实现。在ORB中，这种改进后的FAST称为Oriented FAST。</p>
<p>BRIEF描述子</p>
<p>BRIEF是一种二进制描述子，其描述向量由许多个0和1组成，这里的0和1编码了关键点附近两个随机像素的大小关系，若p像素比q大，则取1，反之取0。BRIEF使用了随机选点的比较，速度非常快，而且由于使用了二进制表达，存储起来也十分方便，适用于实时的图像匹配。原始的BRIEF描述子不具有旋转不变性，因此在图像发生旋转时容易丢失。而ORB在FAST特征点提取阶段计算了关键点的方向，所以可以利用方向信息，计算旋转之后的“Steer BRIEF”特征使得ORB的描述子具有较好的旋转不变性。</p>
<p>特征匹配</p>
<p>特征匹配解决了SLAM中的数据关联问题，即确定当前的路标与之前看到的路标之间的对应关系，通过对图像与图像之间或者图像与地图之间的描述子准确匹配，可以为后续的姿态估计、优化等操作减轻大量负担。最简单的特征匹配方法就是暴力匹配，即对每一个特征点与下一张图片的每一个特征点测量描述子的距离，然后排序取最近的一个作为匹配点。描述子距离表示了两个特征之间的相似程度，不过在实际运用中还可以取不同的距离度量范数。对于浮点类型的描述子，使用欧式距离进行度量即可，而对于二进制的描述子（比如BRIEF），往往用汉明距离（两个二进制串不同位数的个数）作为度量。当特征点数量很大时，暴力匹配的运算量将变得很大，此时，快速近似最近邻算法更加适合于匹配点数量极多的情况。</p>
<p>计算相机运动</p>
<p>特征点提取和匹配可以得到匹配好的点对，接下来的工作就是根据点对估计相机的运动。由于相机原理不同，分为以下三种情况：</p>
<ol>
<li>当相机为单目相机时，只知道2D的像素坐标，因而问题是根据两组2D点估计运动，该问题用对极几何解决。</li>
<li>如果一组为3D，一组为2D，即得到了一些3D点和他们在相机的投影位置，也能估计相机的运动。该问题通过PnP求解。</li>
<li>当相机为单目、RGB-D时，或者通过某种方法得到了距离信息，那么问题就是根据两组3D点估计运动，该问题用ICP（迭代最近点）解决。</li>
</ol>
<p>2D-2D：对极几何</p>
<p>如果从两张图像中取得了5对以上（一般为8对以上）匹配好的特征点，就可以通过这些二维图像点的对应关系，使用对极约束，恢复出两帧之间摄像机的运动。</p>
<p>3D-2D：PnP</p>
<p>PnP是求解3D到2D点对运动的方法。它描述了当知道n个3D空间点及其投影位置时，如何估计相机的位置。3D-2D方法不需要使用对极约束，又可以在很少的匹配点中获得较好的运动估计，是一种最重要的姿态估计方法。</p>
<p>PnP问题有很多种求解方法，比如用3对点估计位姿的P3P、直接线性变换（DLT）、EPnP、UPnP，等等。此外还能使用非线性优化的方式构建最小二乘问题并迭代求解，即光速法平差（BA）。</p>
<p>3D-3D：ICP</p>
<p>和PnP类似，ICP的求解也分为两种方式：利用线性代数的求解（主要是SVD），以及利用非线性优化方式的求解（类似于BA）。</p>
<h1 id="光流法"><a href="#光流法" class="headerlink" title="光流法"></a>光流法</h1><p>光流是一种描述像素随时间在图像之间运动的方法。计算部分像素运动的称为稀疏光流（以Lucas-Kanade光流为代表），计算所有像素的称为稠密光流（以Horn-Schunck光流为代表）</p>
<p>Lucas-Kanade光流</p>
<p>在LK光流中，我们认为来自相机的图像是随时间变化的，图像可以看作时间的函数：$I(t)$，那么在一个$t$时刻，位于$(x,y)$处的像素，它的灰度可以写成</p>
<script type="math/tex; mode=display">
I(x,y,t)</script><p>这种方式把图像看成了关于位置与时间的函数，它的值域就是图像中像素的灰度。现在考虑某个固定的空间点，它在$t$时刻的像素坐标为$x,y$。由于相机的运动，它的图像坐标将发生变化。我们希望估计这个空间点在其他时刻图像中的位置。怎么估计呢？这里引入光流法的基本假设：</p>
<p>灰度不变假设：同一个空间点的像素灰度值在各个图像中是固定不变的。（这是一个很强的假设，事实上，由于物体的材质不同，像素会出现高光和阴影部分，有时，相机会自动调整曝光参数，使得图像整体变亮或变暗。这时光流的结果不一定可靠）</p>
<p>对于$t$时刻位于$(x,y)$处的像素，我们设$t+dt$时刻它运动到$(x+dx,y+dy)$处，由于灰度不变假设，我们有</p>
<script type="math/tex; mode=display">
I(x+dx,y+dy,t+dt)=I(x,y,t)</script><p>对上式左边进行泰勒展开，保留一阶项可得</p>
<script type="math/tex; mode=display">
I(x+dx,y+dy,t+dt)≈I(x,y,z)+\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt</script><p>由于灰度不变假设，所以有</p>
<script type="math/tex; mode=display">
\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt=0</script><p>即</p>
<script type="math/tex; mode=display">
\frac{\partial I}{\partial x}\frac{dx}{dt}+\frac{\partial I}{\partial y}\frac{dy}{dt}=-\frac{\partial I}{\partial t}</script><p>其中，$dx/dt$为像素在$x$轴上的运动速度，$dy/dt$为像素在$y$轴上的运动速度，把它们记为$u,v$。同时${\partial I}/{\partial x}$为图像在该点处$x$方向的梯度，记为$I_x$，${\partial I}/{\partial y}$为图像在该点处$x$方向的梯度，记为$I_y$。把图像对时间的变化量记为$I_t$，写成矩阵形式，有</p>
<script type="math/tex; mode=display">
\begin{bmatrix}   I_x & I_y \end{bmatrix}
\begin{bmatrix} u \\ v \end{bmatrix}=-I_t</script><p>我们想计算的是像素的运动$u, v$，但是上式是带有两个变量的一次方程，仅凭它无法计算出$u, v$。故必须引入额外的约束来计算$u, v$，因此在LK光流中，我们假设某一个窗口的像素具有相同的运动。</p>
<p>现考虑一个大小为$w×w$的窗口，它含有$w^2$数量的像素。该窗口内像素具有同样的运动，因此我们共有$w^2$个方程：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}   I_x & I_y \end{bmatrix}_k
\begin{bmatrix} u \\ v \end{bmatrix}=-I_{tk},\quad k=1,\cdots,w^2</script><p>记：</p>
<script type="math/tex; mode=display">
A=\begin{bmatrix}  [ I_x,I_y]_1 \\ \vdots \\ [ I_x,I_y]_k \end{bmatrix}，b=\begin{bmatrix}  I_{t1} \\ \vdots \\ I_{tk} \end{bmatrix}</script><p>于是，整个方程为</p>
<script type="math/tex; mode=display">
A
\begin{bmatrix} u \\ v \end{bmatrix}=-b</script><p>这是一个关于$u,v$的超定线性方程，传统解法是求最小二乘解，上式最小二乘解为</p>
<script type="math/tex; mode=display">
\begin{bmatrix} u \\ v \end{bmatrix}^*=-(A^TA)^{-1}A^Tb</script><p>这样就得到了像素在图像间的运动速度$u,v$。</p>
<p>光流法可以加速基于特征点的视觉里程计算法，避免计算和匹配描述子的过程，但要求相机运动比较平滑（或采集频率较高）。</p>
<h1 id="直接法"><a href="#直接法" class="headerlink" title="直接法"></a>直接法</h1><p>在光流中，我们会首先追踪特征点的位置，再根据这些位置确定相机的运动。这种一步两步走的方案很难保证全局的最优性。如果在后一步中调整前一步的结果，即后一步以前一步作为初始值的假设，调整光流的计算结果，这就是直接法。</p>
<p>考虑某个空间点P和两个时刻的相机。P的世界坐标为$[X,Y,Z]$，它在两个相机上成像，记像素坐标为$p_1,p_2$。我们的目标是求第一个相机到第二个相机的相对位姿变换。我们以第一个相机为参考系，设第二个相机的旋转和平移为$R,t$（对应李群为$T$）。同时，两相机的内参相同，记为$K$，列写完整的投影方程为：p_1=</p>
<script type="math/tex; mode=display">
p_1=\begin{bmatrix} u \\ v \\ 1\end{bmatrix}_1=\frac{1}{Z_1}KP</script><script type="math/tex; mode=display">
p_1=\begin{bmatrix} u \\ v \\ 1\end{bmatrix}_2=\frac{1}{Z_2}K(RP+t)=\frac{1}{Z_2}K(TP)_{1:3}</script><p>其中，$Z_1$是$P$的深度，$Z_2$是$P$在第二个相机下的深度，也就是$RP+t$的第3个坐标值。由于$T$只能和齐次坐标相乘，所以我们乘完之后要取出前三个元素。</p>
<p>在特征点法中，由于我们通过匹配描述子知道了$p_1,p_2$的像素位置，所以可以计算重投影的位置。但在直接法中，由于没有特征匹配，我们无从知道哪一个$p_2$与$p_1$对应着同一个点。直接法的思路就是根据当前相机的位姿估计值寻找$p_2$的位置。但若相机的位姿不够好，$p_2$的外观和$p_1$会有明显差别。为了减小这个差别，需要优化相机的位置，寻找与$p_1$更相似的$p_2$。这同样可以通过解一个优化问题完成，但此时最小化的不是重投影误差，而是光度误差，也就是$P$的两个像素的亮度误差：</p>
<script type="math/tex; mode=display">
e=I_1(p_1)-I_2(p_2)</script><p>这里的$e$是一个标量，优化目标为该误差的二范数，暂时取不加权的形式，为：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{T}J(T)=\mid\mid e \mid\mid^2</script><p>能够做这种优化的理由是基于灰度不变假设（假设一个空间点在各视角下成像的灰度是不变的）。我们有$N$个空间点$P_i$，那么，整个相机位姿估计问题变为</p>
<script type="math/tex; mode=display">
\mathop{min}\limits_{T}J(T)=\sum\limits_{i=1}^N e_i^Te_i， e_i=I_1(p_1,i)-I_2(p_2,i)</script><p>这里的优化变量是相机位姿$T$，而不像光流那样优化各个特征点的运动。为了求解这个优化问题，我们关心误差$e$是如何随着相机位姿$T$变化的。因此，定以两个中间变量：</p>
<script type="math/tex; mode=display">
q=TP,\\u=\frac{1}{Z_2}Kq</script><p>这里的$q$为$P$在第二个相机坐标系下的坐标，而$u$为它的像素坐标。显然$q$是$T$的函数，$u$是$q$的函数。考虑李代数的左扰动模型，利用一阶泰勒展开，因为：</p>
<script type="math/tex; mode=display">
e(T)=I_1(p_1)-I_2(u)</script><p>所以</p>
<script type="math/tex; mode=display">
\frac {\partial e}{\partial T}=\frac {\partial I_2}{\partial u}\frac {\partial u}{\partial q}\frac {\partial q}{\partial {\delta \xi}}\delta \xi</script><p>其中$\delta \xi$为$T$的左扰动。可以看到，一阶导数由于链式法则分成了3项，而这三项都是容易计算的：</p>
<ol>
<li><p>$\frac {\partial I_2}{\partial u}$为$u$处的像素梯度。</p>
</li>
<li><p>$\frac {\partial u}{\partial q}$为投影方程关于相机坐标系下的三维点的导数。记$q=[X,Y,Z]^T$，根据上面的推导，导数为</p>
<script type="math/tex; mode=display">
\frac {\partial u}{\partial q}=\begin{bmatrix} \frac {\partial u}{\partial X} &\frac {\partial u}{\partial Y} &\frac {\partial u}{\partial Z} \\ \frac {\partial v}{\partial X} &\frac {\partial v}{\partial Y} &\frac {\partial v}{\partial Z} \end{bmatrix}=\begin{bmatrix} \frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} \\ 0 &\frac{f_y}{Z} & -\frac{f_yY}{Z^2}  \end{bmatrix}</script></li>
</ol>
<ol>
<li>$\frac {\partial q}{\partial {\delta \xi}}$为变换后的三维点对变换的导数<script type="math/tex; mode=display">
\frac {\partial q}{\partial {\delta \xi}}=[I,-q\hat{}]</script></li>
</ol>
<p>在实践中，由于后两项只与三维点$q$有关，而与图像无关，我们常常把它合并在一起：</p>
<script type="math/tex; mode=display">
\frac {\partial u}{\partial {\delta \xi}}=\begin{bmatrix} \frac{f_x}{Z} & 0 & -\frac{f_xX}{Z^2} &-\frac{f_xX}{Z^2}&f_x+\frac{f_xX^2}{Z^2}&-\frac{f_xY}{Z}\\ 0 &\frac{f_y}{Z} & -\frac{f_yY}{Z^2}&-f_y-\frac{f_yY}{Z^2} & \frac{f_yXY}{Z^2} & \frac{f_yX}{Z}\end{bmatrix}</script><p>于是推导出误差相对于李代数的雅可比矩阵：</p>
<script type="math/tex; mode=display">
J=-\frac {\partial I_2}{\partial u}\frac {\partial u}{\partial {\delta \xi}}</script><p>对于$N$个点的问题，我们可以用这种方法计算优化问题的雅可比矩阵，然后使用高斯牛顿法或列文伯格-马夸尔特方法计算增量，迭代求解。至此我们推导了直接法估计相机位姿的整个流程。</p>
<p>在上面的推导中，$P$是一个已知位置的空间点，它是怎么来的呢？在$RGB-D$ 相机下，我们可以把任意像素反投影到三维空间，然后投影到下一幅图像中；如果在双目相机中，同样可以根据视差来计算像素的深度；如果是单目相机，还须考虑$P$的深度带来的不确定性。</p>
<p>根据$P$的来源，可以把直接法分类：</p>
<ol>
<li>$P$来自于稀疏关键点，我们称之为稀疏直接法。这种稀疏直接法不必计算描述子，并且只使用数百个像素，因此速度最快，但只能计算稀疏的重构。</li>
<li>$P$来自部分像素，在误差相对于李代数的雅可比矩阵中，如果像素的梯度为0，那么整个雅可比矩阵为0，不会对计算运动增量有任何贡献。因此可以考虑只使用带有梯度的像素点，舍弃像素梯度不明显的地方。这称为半稠密的直接法，可以重构一个半稠密的结构。</li>
<li>$P$为所有像素，称为稠密直接法。稠密重构需要计算所有像素，因此多半不能在现有的CPU上实时计算，需要GPU的加速。其中，像素梯度不明显的地方，在运动估计中不会有太大贡献，在重构时也会难以估计位置。</li>
</ol>
<p>可以看到，从稀疏到稠密重构，都可以用直接法计算。它们的计算量是逐渐增长的。稀疏方法可以快速地求解相机位姿，而稠密方法可以建立完整地图。</p>
]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>LXP广场</title>
    <url>/2020/03/19/%E7%95%99%E8%A8%80%E6%9D%BF/</url>
    <content><![CDATA[<p>现在开始，这里叫做LXP广场~<br><a id="more"></a><br><img src="/2020/03/19/%E7%95%99%E8%A8%80%E6%9D%BF/2.jpg" alt="logo"></p>
]]></content>
  </entry>
  <entry>
    <title>最优估计学习备忘四</title>
    <url>/2020/03/19/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E5%9B%9B/</url>
    <content><![CDATA[<h1 id="线性卡尔曼滤波的实施"><a href="#线性卡尔曼滤波的实施" class="headerlink" title="线性卡尔曼滤波的实施"></a>线性卡尔曼滤波的实施</h1><p>在理想的情况下，如果数学模型与物理现实一致，卡尔曼滤波得到的是无偏线性最小方差估计，而且随着测量数据的增加，卡尔曼滤波估计逐渐趋近状态的期望，误差方差逐渐趋于稳态值。但在实际应用中卡尔曼滤波会出现各种问题，最常见的是滤波估计值偏离实际的状态值，如果随着时间的推移这种偏离越来越大甚至趋于无穷大，就称滤波发生了发散。同时一般在滤波估计发生偏移和发散时，滤波估计的方差也发生偏移和发散。<br><a id="more"></a><br>滤波发散的原因有三个，一是模型发散，即系统的数学模型不是一致完全可控和一致完全可测的，滤波不具有一致渐进稳定性，抑制模型发散的基本方法是改变系统的结构及参数，使系统在随机作用下有一致完全可控可测；二是数值发散，由于计算机的字长是有限的，这就使得滤波递推中的每一步计算都有截断误差，这使得误差方差逐渐失去正定性甚至对称性，滤波增益的计算值与理论值的差异越来越大，从而导致发散，抑制数值发散的最基本方法是采用双字长运算，这样可以有效减少数字的损失，对于滤波问题，抑制数值发散还可以在滤波过程中将方差阵分解（使用平方根滤波、$UDU^T$分解滤波），通过分解后的矩阵代替原来的方差矩阵进行递推，此外当滤波的方差很大甚至无穷大时，将导致滤波无法计算，这时可将方差的逆矩阵来代替原方差进行滤波递推（信息滤波）；三是由于对物理现实认识不足，对实际系统的过程缺乏完整的了解或足够的统计数据，是建立了不准确的数学模型，针对此类问题，可使用有色噪声下卡尔曼滤波（将有色噪声转化为白噪声）、扩展的卡尔曼滤波（一定程度上减小方程线性化带来的模型误差）、自适应的卡尔曼滤波（实时地调整和修正系统模型和噪声地方差矩阵，以达到模型地自适应地与现实相吻合）来解决对应的问题。</p>
<h2 id="平方根滤波"><a href="#平方根滤波" class="headerlink" title="平方根滤波"></a>平方根滤波</h2><p>由于计算机舍入误差的存在，使得滤波方差矩阵$D_{\hat{X}}(k,k-1)$和$D_{\hat{X}}(k)$的数值失去非负定性甚至失去对称性，从而导致$K_k$的计算失真，并产生发散现象。计算误差主要是由于过大的数值与过小的数值相加减过程中造成的，为了避免运算过程中的数值过大或过小，我们可以将其平方根分解（对数值开方，例如0.01开方成0.1）。在卡尔曼滤波递推计算中，我们可以对方差矩阵做平方根分解，对称正定矩阵$D$可以唯一地分解为一个下三角矩阵$S$及其转置$S^T$的乘积，即$D=SS^T$，矩阵$S$即为矩阵$D$的平方根矩阵。在计算矩阵$S$只需要保留矩阵$D$一半的字长，$SS^T$就可以达到很高的精度。正定矩阵可使用$Cholesky$分解法。</p>
<p>首先将状态方差矩阵做平方根分解：</p>
<script type="math/tex; mode=display">
D_{\hat{X}}=S_kS_k^T，D_{\hat{X}}(k,k-1)=S_{k,k-1}S_{k,k-1}^T</script><p>然后在卡尔曼滤波的基本方程中，以$S_k$和$S_{k.k-1}$的递推关系式来代替原来的$D_{\hat{X}}$和$D_{\hat{X}}(k,k-1)$</p>
<p>的递推关系式，这样就可以保证方差矩阵在任意时刻都是对称非负定的。这里不加推导地给出平方根滤波递推公式</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>一步预测</th>
<th>状态预测</th>
<th>$\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)$</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>预测方差</td>
<td>$D_{\hat{X}}(k,k-1)=S_{k,k-1}S_{k,k-1}^T$</td>
</tr>
<tr>
<td>测量更新</td>
<td>状态滤波</td>
<td>$\begin{cases}\hat{X}^0(k)=\hat{X}(k-1) \\ S_k^0=S_{k,k-1} \end{cases}$，$\hat{X}^j=\hat{X}^{j-1}(k)+K_k^j(Z_j(k)-h_j^k\hat{X}^{j-1}(j=1,2,\cdots,l)$</td>
</tr>
<tr>
<td></td>
<td>增益矩阵</td>
<td>$a_k=(h_k^1S_k^{j-1})，b_k=[(a_k)^Ta_k+D_\Delta^j(k)]，K_k^{j}=b_k^{-1}S_k^{j-1}a_k^T$</td>
</tr>
<tr>
<td></td>
<td>平方根</td>
<td>$S_k^j=S_k^{j-1}-\frac{(b_k+\sqrt(D_\Delta^j(k)b_k)}{a_k^Ta_k}K_k^ja_k^T$</td>
</tr>
<tr>
<td></td>
<td>滤波方差</td>
<td>$D_{\hat{X}(k)}=S_kS_k^T,S_k=S_k^l$</td>
</tr>
</tbody>
</table>
</div>
<p>矩阵分解地方法有多种，除了可以对方差矩阵$D_{\hat{X}}(k,k-1)$和$D_{\hat{X}}(k)$进行平方根分解外，还可以对其进行$UDU^T$分解来传递方差。</p>
<h2 id="扩展的卡尔曼滤波"><a href="#扩展的卡尔曼滤波" class="headerlink" title="扩展的卡尔曼滤波"></a>扩展的卡尔曼滤波</h2><p>对线性模型而言，卡尔曼滤波估计是最优估计，但严格地说现实中的系统并非都是线性的，由于对非线性模型的估计难度大大增加，所以通常将非线性模型线性化后进行卡尔曼滤波估计，所以这样得到的估计实际上是一种近似估计方法。这里介绍的扩展的卡尔曼滤波$EKF$将非线性模型中的状态方程在$\hat{X}(k-1)$处用泰勒公式展开并舍去二阶项和高阶项进行线性化；将观测方程在$\hat{X}(k,k-1)$处用泰勒公式展开并舍去二阶项和高阶项进行线性化，然后用线性系统的卡尔曼滤波公式进行估计。</p>
<p>非线性离散时间系统</p>
<script type="math/tex; mode=display">
X(k)=f(X(k-1),e(k-1))\\Z(k)=h(X(k))+\Delta(k)</script><p>随机模型</p>
<script type="math/tex; mode=display">
E[e(k)]=0，E[\Delta(k)]=0,\\cov[e(k)，e(j)]=D_e(k)\delta(k-j)\\cov[ \Delta(k)，\Delta(j)]=D_e(k)\delta(k-j)\\cov[e(k),\Delta(j)]=0</script><p>初始状态为</p>
<script type="math/tex; mode=display">
E[X(0)]=\mu_X(0)，var[X(0)]=D_X(0)</script><p>现在将状态方程在近似值$X_{(0)}(k-1)$和$e(k-1)=0$处用泰勒公式展开并舍去高阶项</p>
<script type="math/tex; mode=display">
X(k)=f(X_{(0)}(k-1),0)+\frac{\partial f}{\partial X(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0}(X(k-1))-X_{0}(k-1)\\+\frac{\partial f}{\partial e(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0}(e(k-1))</script><p>令</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial X(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0})=\Phi_{k,k-1}</script><script type="math/tex; mode=display">
\frac{\partial f}{\partial e(k-1)}\mid_{X(k-1)=X_{(0)}(k-1)\\e(k-1)=0})=\Gamma(k-1)</script><script type="math/tex; mode=display">
\eta(k-1)=\Gamma(k-1)e(k-1)</script><p>有线性化后的状态方程</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+[f(X_{(o)}(k-1),0)-\Phi_{k,k-1}X_{(o)}(k-1)]+\eta(k-1)</script><p>将观测方程在近似值$X_{(0)}(k)$处用泰勒公式展开并舍去高阶项有</p>
<script type="math/tex; mode=display">
Z(k)=\frac{\partial f}{\partial X(k)}\mid_{X(k)=X_{(0)}(k)}(X_k-X_{(0)}(k))+h(X_{(0)}(k))+\Delta(k)</script><p>设</p>
<script type="math/tex; mode=display">
H(k)=\frac{\partial f}{\partial X(k)}\mid_{X(k)=X_{(0)}(k)}</script><p>则有线性化后的观测方程</p>
<script type="math/tex; mode=display">
Z(k)=H(k)X_k[h(X_{(0)}(k))-H(k)X_{(0)}(k)]+\Delta(k)</script><p>这时的随机模型为</p>
<script type="math/tex; mode=display">
E[\eta(k)]=0,E[\Delta(k)]=0\\cov[\eta(k),\eta(j)]=\Gamma(k)D_e(k)\Gamma^T(k)\delta(k-j)\\cov[\Delta(k),\Delta(j)]=\Gamma(k)D_{\Delta}(k)\delta(k-j)\\cov[\eta(k),\Delta(j)=0]</script><p>以上线性化模型的卡尔曼滤波为</p>
<p>1）一步预测</p>
<script type="math/tex; mode=display">
\hat{X}(k)=\Phi_{k,k-1}\hat{X}(k-1)+f(X_{(o)}(k-1),0)-\Phi_{k,k-1}X_{(o)}(k-1)</script><p>若取$X_{(0)}(k-1)=\hat{X}(k-1)$并代入上式得到</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=f(\hat{X}(k-1),0)</script><p>其方差为</p>
<script type="math/tex; mode=display">
D_{\hat{X}}(k,k-1)=\Phi_{k,k-1}D_{\hat{X}(k-1)}\Phi_{k,k-1}^T+\Gamma(k-1)D_{e(k-1)}\Gamma(k-1)^T</script><p>预测残差为</p>
<script type="math/tex; mode=display">
V_z(k)=Z(k)-[H(k)\hat{X}(k,k-1)+[h(X_{(0)}(k))-H(k)X_{(0)}(k)]]</script><p>若取$X_{(0)}(k)=\hat{X}(k,k-1)$并代入上式得到</p>
<script type="math/tex; mode=display">
V_z(k)=Z(k)-h(\hat{X}(k,k-1))</script><p>测量更新</p>
<script type="math/tex; mode=display">
\hat{X}(k)=X(k,k-1)+K_kV_z(k)\\D_{\hat{X}}(k)=(I-K_kH_k)D_{\hat{X}}(k,k-1)</script><p>增益矩阵</p>
<script type="math/tex; mode=display">
K_k=D_{\hat{X}}(k,k-1)H_k^T[H_kD_{\hat{X}}H_k^T+D_{\Delta}(k)]^{-1}</script><p>扩展的卡尔曼滤波将非线性的状态方程在$\hat{X}(k-1)$处进行线性化，将观测方程在$\hat{X}(k,k-1)$处线性化，用线性卡尔曼滤波进行估计，虽然从估计公式上看体现出了非线性函数，但实质仍然是线性估计。</p>
]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘三</title>
    <url>/2020/03/18/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%B8%89/</url>
    <content><![CDATA[<h1 id="最优估计实践（线性卡尔曼滤波）"><a href="#最优估计实践（线性卡尔曼滤波）" class="headerlink" title="最优估计实践（线性卡尔曼滤波）"></a>最优估计实践（线性卡尔曼滤波）</h1><p>卡尔曼滤波采用状态空间来描述系统，在经典估计理论中加入了状态方程，通过对被提取信号有关的量测来估计所需要的信号（状态变量）。在估计中利用的信息有：状态方程、观测方程、系统噪声、量测噪声和初始状态的统计特性，即已知系统和量测的数学模型，实时获得系统状态变量的最优估计。由于状态可以是多维，卡尔曼滤波的维数不再局限于一维，而且卡尔曼滤波算法采用递推形式，所以也可以处理非平稳的随机过程。卡尔曼滤波是一种线性，无偏且方差最小的最优估计方法。对于计算机运算来说，卡尔曼滤波的递推使其运算量和存储量大为减少，容易满足实时估计的要求。<br><a id="more"></a></p>
<h2 id="线性离散系统的卡尔曼滤波"><a href="#线性离散系统的卡尔曼滤波" class="headerlink" title="线性离散系统的卡尔曼滤波"></a>线性离散系统的卡尔曼滤波</h2><p>离散线性系统的函数模型为</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)\\Z(k)=H_kX(k)+\Delta(k)</script><p>随机模型为</p>
<script type="math/tex; mode=display">
\begin{cases}E[e(k)]=0,cov[e(k),e(j)]=D_e(k)\delta(k-j)\\E[\Delta(k)]=0,cov[\Delta(k),\Delta(j)]=D_\Delta(k)\delta(k-j)\\cov[e(k),\Delta(j)]=0 \end{cases}</script><p>式中，$\Phi_{k,k-1}$为$t_{k-1}$时刻至$t_k$时刻的转移矩阵；$\Gamma_{k,k-1}$为系统噪声驱动阵；$e(k-1)$为系统激励噪声序列；$H_k$为量测矩阵；$\Delta(k)$为量测噪声序列。</p>
<p>初始状态$X_0$的统计特性为</p>
<script type="math/tex; mode=display">
E(X(0))=\hat{X}(0)=\mu_x(0),var(X(0))=D_{\hat{X}}(0)\\cov[X(0),e(k)]=0,cov[X(0),\Delta(k)]=0</script><p>下面给出对状态$X(k)$的卡尔曼滤波估计及其推导过程。</p>
<h3 id="基于最小方差准则的推导"><a href="#基于最小方差准则的推导" class="headerlink" title="基于最小方差准则的推导"></a>基于最小方差准则的推导</h3><p>1）推导过程</p>
<p>滤波的一步预测</p>
<p>与最小二乘估计的数学模型不同的是动态系统的数学描述中增加了状态方程</p>
<script type="math/tex; mode=display">
X(k)=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)</script><p>状态方程建立了$X(k)$与$X(k-1)$的关系，因此我们可以在得到观测值$Z(k)$前就得到$X(k)$的一些先验信息。由最小方差估计可以根据$k-1$个量测$Z(1),Z(2),\cdots,Z(k-1)$对$X(k)$做最小方差估计，记为$\hat{X}(k,k-1)$</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=E[X(k)/Z(1)Z(2)\cdots Z(k-1)] \\=E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)]</script><p>上式的条件期望也可表示为</p>
<script type="math/tex; mode=display">
E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)] \\=\iint(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))f([x(k-1),e(k-1)/z(1)z(2)\cdots  \\z(k-1))dx(k-1)de(k-1)])</script><p>因为知道$e(k-1)$只影响$X(k)$，所以$e(k-1)$与$X(k-1)$以及$Z(1),Z(2),\cdots,Z(k-1)$都不相关，所以</p>
<script type="math/tex; mode=display">
f([x(k-1),e(k-1)]/z(1)z(2)\cdots z(k-1)) \\=f(e(k-1))f(x(k-1))/z(1)z(2)\cdots z(k-1))</script><p>此式代入上一式可得</p>
<script type="math/tex; mode=display">
E[(\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1))/Z(1)Z(2)\cdots Z(k-1)] \\=\Phi_{k,k-1}E(X(k-1)/Z(1)Z(2)\cdots Z(k-1))</script><p>所以$\hat{X}(k,k-1)$为（$E(e(k-1)=0)$）</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=\Phi_{k,k-1}E(X(k-1))/Z(1)Z(2)\cdots Z(k-1)</script><p>由最小方差估计知道，上式中的$E(X(k-1)/Z(1)Z(2)\cdots Z(k-1))$是对$X(k-1)$的最小方差估计，即为$\hat{X}(k-1)$，即有</p>
<script type="math/tex; mode=display">
\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)</script><p>上式表明$Z(1)Z(2)\cdots Z(k-1)$对$X(k)$的最小方差估计$\hat{X}(k,k-1)$为状态转移矩阵$\Phi_{k,k-1}$乘以最小方差估计$\hat{X}(k-1)$。$\hat{X}(k,k-1)$为未有观测值$Z(k)$前对$X(k)$的预测，称为$X(k)$的一步预测，其预测误差为</p>
<script type="math/tex; mode=display">
\Delta\hat{X}(k,k-1)=X(k)-\hat{X}(k,k-1) \\=\Phi_{k,k-1}X(k-1)+\Gamma_{k,k-1}e(k-1)-\Phi_{k,k-1}\hat{X}(k-1) \\=\Phi_{k,k-1} \Delta\hat{X}(k-1)+\Gamma_{k,k-1}e(k-1)</script><p>其中</p>
<script type="math/tex; mode=display">
\Delta\hat{X}(k-1)=X(k-1)-\hat{X}(k-1)</script><p>$\Delta\hat{X}(k,k-1)$的期望为</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=\Phi_{k,k-1} E(\Delta\hat{X}(k-1))+\Gamma_{k,k-1}E(e(k-1))</script><p> 因为$E(e(k-1))=0$，$E(\Delta\hat{X}(k-1))$=0（后面证明$\hat{X}(k-1)$是$X(k-1)$的无偏估计），所以有</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=0</script><p>即</p>
<script type="math/tex; mode=display">
E(\hat{X}(k,k-1))=E(X(k))</script><p>即证明得到 $\hat{X}(k,k-1)$ 是$X(k)$的无偏估计。</p>
<p>滤波的测量更新</p>
<p>将一步预测$\hat{X}(k,k-1)$代入观测方程，忽略观测误差可以得到观测值$Z(k)$的预测值</p>
<script type="math/tex; mode=display">
\hat{Z}(k,k-1)=H_k\hat{X}(k,k-1)</script><p>预测值与实际观测值的差异为</p>
<script type="math/tex; mode=display">
V_z(k,k-1)=Z(k)-\hat{Z}(k,k-1) \\=H_kX(k)+\Delta(k)-H_k\hat{X}(k,k-1) \\=H_k\Delta\hat{X}(k,k-1)+\Delta(k)</script><p>$V_z(k,k-1)$在滤波理论中称为预测残差（也称新息）。从上式可以看出，预测残差包含一步预测误差和观测误差。$V_z(k,k-1)$的期望为</p>
<script type="math/tex; mode=display">
E(V_z(k,k-1))=H_kE(\Delta\hat{X}(k,k-1))+E(\Delta(k))</script><p>因为$E(e(k-1))$为零，$\Delta\hat{X}(k,k-1)$的期望也为零，所以</p>
<script type="math/tex; mode=display">
E(V_z(k,k-1))=0</script><p>若将$V_z(k,k-1)$看作观测值，那么式子</p>
<script type="math/tex; mode=display">
V_z(k,k-1)=H_k\Delta\hat{X}(k,k-1)+\Delta(k)</script><p>就是将$\Delta\hat{X}(k,k-1)$视为未知参数的新的观测方程，其中</p>
<script type="math/tex; mode=display">
E[\Delta(k)]=0,cov[\Delta(k)]=D_\Delta(k)\\E(\Delta\hat{X}(k,k-1))=0,cov(\Delta\hat{X}(k,k-1))=D_{\hat{X}(k,k-1)}</script><p>根据线性最小方差估计理论可以直接得到$\Delta\hat{X}(k,k-1)$的线性最小方差估计，记为$\Delta\hat{\hat{X}}(k,k-1)$，其值为</p>
<script type="math/tex; mode=display">
\Delta\hat{\hat{X}}(k,k-1)=E(\Delta\hat{X}(k,k-1)) \\+D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}(V_z(k,k-1)-H_kE(\Delta\hat{X}(k,k-1)))</script><p>由于</p>
<script type="math/tex; mode=display">
E(\Delta\hat{X}(k,k-1))=0</script><p>有</p>
<script type="math/tex; mode=display">
\Delta\hat{\hat{X}}(k,k-1)=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}V_z(k,k-1)</script><p>又因为</p>
<script type="math/tex; mode=display">
X(k)=\Delta\hat{X}(k,k-1)+\hat{X}(k,k-1)</script><p>所以用$\Delta\hat{X}(k-1)$的线性最小方差估计值$\Delta\hat{\hat{X}}(k,k-1)$替代上式中的$\Delta\hat{X}(k-1)$可以得到由$V_z(k,k-1)$对$X(k)$的估计$\hat{X}(k)$ </p>
<script type="math/tex; mode=display">
\hat{X}(k)=\Delta\hat{X}(k,k-1)+\hat{X}(k,k-1) \\=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}V_z(k,k-1)+\hat{X}(k,k-1)</script><p>由于$V_z(k,k-1)$是由观测值$z(k)$得到，具有观测值的信息，所以上式即为观测值对$\hat{X}(k,k-1)$ 的更新，记</p>
<script type="math/tex; mode=display">
K_k=D_{\hat{X}}(k,k-1)H_k^T(H_kD_{\hat{X}}(k,k-1)H_k^T+D_{\Delta}(k))^{-1}</script><p>有</p>
<script type="math/tex; mode=display">
\hat{X}(k)=K_kV_z(k,k-1)+\hat{X}(k,k-1)\\=\hat{X}(k,k-1)+K_k(Z(k)-H_k\hat{X}(k,k-1))</script><p>至此，我们得到了对$X(k)$的一步预测和测量更新估计的递推公式。</p>
<script type="math/tex; mode=display">
\begin{cases}\hat{X}(k,k-1)=\Phi_{k,k-1}\hat{X}(k-1)\\ \hat{X}(k)=\hat{X}(k,k-1)+K_k(Z(k)-H_k\hat{X}(k,k-1))\end{cases}</script><p>$\hat{X}(k)$的期望为</p>
<script type="math/tex; mode=display">
E(\hat{X}(k))=K_kE(V_z(k,k-1))+E(\hat{X}(k,k-1))=E(\hat{X}(k,k-1))=E(X(k))</script><p>即$X(k)$的观测值更新为无偏估计。</p>
<p>以上就是根据最小方差准则得到的卡尔曼滤波器。卡尔曼滤波器还可以用正交投影法和递推最小二乘准则推导，正交投影推导可以使我们看到卡尔曼滤波器的数学内涵：$\hat{X}(k)$是$X(k)$在$Z(k)$上的正交投影，在递推最小二乘的推导中，将一步预测的$\hat{X}(k,k-1)$看作虚拟观测值，与$Z(k)$一并组成观测方程进行最小二乘平差，最后得到的$X(k)$与最小方差估计$\hat{X}(k)$相等，这也说明卡尔曼滤波估计是残差平方和最小的估计。</p>
<h3 id="滤波器稳定性判别"><a href="#滤波器稳定性判别" class="headerlink" title="滤波器稳定性判别"></a>滤波器稳定性判别</h3><p>由于滤波方程由系统的状态方程和观测方程给出，滤波的稳定性应该与随机线性系统的结构和参数有关。如果线性系统是随机一致完全可控和随机一致完全可测的，那么卡尔曼滤波器是一致渐进稳定的。</p>
<p>离散时不变系统一致完全随机可控的条件为</p>
<script type="math/tex; mode=display">
rank[\Gamma \quad \Phi\Gamma  \quad  \cdots\quad \Phi^{N-1}\Gamma]=n</script><p>离散时不变系统一致完全随机可测的条件为</p>
<script type="math/tex; mode=display">
rank[H \quad H\Phi  \quad  \cdots\quad H\Phi^{N-1}]^T=n</script>]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘二</title>
    <url>/2020/03/18/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%BA%8C/</url>
    <content><![CDATA[<h1 id="动态系统的数学模型及性质"><a href="#动态系统的数学模型及性质" class="headerlink" title="动态系统的数学模型及性质"></a>动态系统的数学模型及性质</h1><p>备忘一介绍了参数估计的数学模型，它描述观测值与参数之间的关系，是静态估计模型。静态估计模型不考虑研究对象的状态在时间上的变化，但生活中大量的研究对象是一个动态系统，在估计时我们必须考虑研究对象的运动变化规律，所以在对动态系统的估计中，除了观测方程以外，还需要对动态系统的物理变化规律或者动力学状态进行数学描述，即描述系统的“输入”与系统“状态”之间的关系，所以称为状态方程。为了研究系统的特性我们对系统进行观测，观测量即为系统的输出，所以观测方程描述了输出与状态之间的关系。状态方程和观测方程构成了动态系统的数学模型。</p>
<a id="more"></a>
<h2 id="状态方程及其对应的解"><a href="#状态方程及其对应的解" class="headerlink" title="状态方程及其对应的解"></a>状态方程及其对应的解</h2><p>设$X(t)$为状态变量，$A(t)$、$B(t)$ 为系数矩阵，$u(t)$ 为输入矩阵，$X(t_0)=X_0$为系统初值</p>
<p>1)若$A(t)$不随时间变化，即为常数矩阵$A$，则有线性时不变齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)</script><p>根据常微分方程的解法并由初始条件可以得到</p>
<script type="math/tex; mode=display">
X(t)=e^{A·(t-t_0)}X(t_0)=e^{A·(t-t_0)}X_0</script><p>现设</p>
<script type="math/tex; mode=display">
\Phi(t-t_0)=e^{A·(t-t_0)}</script><p>则有解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)</script><p>上式说明齐次状态方程的解实质是初始状态$X(t_0)$从初始时刻到时刻$t$系统运动状态的转移，所以$\Phi(t-t_0)$称为状态转移矩阵。</p>
<p>2)连续时不变系统（连续定常系统，$A$、$B$为常数矩阵）非齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+\mathop{B}\mathop{u}(t)</script><p>解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)+\int_{t_0}^te^{A(t-\tau)}·Bu(\tau)d\tau</script><p>上式表明系统在任何时刻的状态取决于系统的初始状态和从初始时刻以后的输入。</p>
<p>3)若动态系统无输入或者不考虑系统的输入，无输入的状态方程为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}(t)\mathop{X}(t)</script><p>上式表明系统本身在无外力的作用下自由运动，这样的状态方程称为齐次微分方程。</p>
<p>设上式的解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t,t_0)X(t_0)</script><p>（注意这时的转移矩阵不是$\Phi(t-t_0)$ ，代入状态方程有  </p>
<script type="math/tex; mode=display">
\mathop{\Phi}\limits^{·}(t,t_0)X(t_0)=A(t)\Phi(t,t_0)X(t_0)</script><p>因此有</p>
<script type="math/tex; mode=display">
\begin{cases}\mathop{\Phi}\limits^{·}(t,t_0)=A(t)\Phi(t,t_0)\\\Phi(t,t_0)=I \end{cases}</script><p>求得满足上述条件的$\Phi(t,t_0)$，$X(t)=\Phi(t,t_0)X(t_0)$即为状态方程的解。</p>
<p>4）连续时变系统非齐次状态方程</p>
<script type="math/tex; mode=display">
\mathop{X}\limits_{n×1}’(t)=\mathop{A}\limits_{n×n}(t)\mathop{X}\limits_{n×1}(t)+\mathop{B}\limits_{n×p}(t)\mathop{u}\limits_{p×1}(t)</script><p>解为</p>
<script type="math/tex; mode=display">
X(t)=\Phi(t-t_0)X(t_0)+\int_{t_0}^t\Phi(t,\tau)·B(\tau)u(\tau)d\tau</script><h2 id="连续线性动态系统的函数模型与随机模型"><a href="#连续线性动态系统的函数模型与随机模型" class="headerlink" title="连续线性动态系统的函数模型与随机模型"></a>连续线性动态系统的函数模型与随机模型</h2><p>在对动态系统进行描述时，总有一些未知的不确定的因素，故引入系统噪声$e(t)$（系统误差）。为研究系统的运动规律，需要对系统进行观测，其观测量设为$Z(t)$，观测噪声设为$\Delta(t)$（观测误差）。</p>
<p>1）当系统为时不变系统时，函数模型为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+\mathop{B}\mathop{u}(t)+Ce(t)\\Z(t)=HX(t)+Gu(t)+\Delta(t)</script><p>2）当动态系统没有控制输入或者不考虑系统的输入时，函数模型为</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}(t)\mathop{X}(t)+Ce(t)\\Z(t)=H(t)X(t)+\Delta(t)</script><p>3)当系统为时不变且没有控制输入时</p>
<script type="math/tex; mode=display">
\mathop{X}’(t)=\mathop{A}\mathop{X}(t)+Ce(t)\\Z(t)=HX(t)+\Delta(t)</script><p>在解决现实问题时，通常假设系统噪声和观测噪声为零均值白噪声，且它们之间完全不相关。</p>
<h2 id="离散线性动态系统的函数模型与随机模型"><a href="#离散线性动态系统的函数模型与随机模型" class="headerlink" title="离散线性动态系统的函数模型与随机模型"></a>离散线性动态系统的函数模型与随机模型</h2><p>对动态系统的观测是基于某些离散的时间点上的观测，所以在对动态系统的估计需要将其状态方程离散化得到离散模型从而便于计算机处理数据。对连续动态系统离散化可以采用两种方法：1）是求得常微分方程得解析解，也就是求得状态转移矩阵，通过状态在时间上的转移从而得到各个离散时刻的系统状态值。2）是根据动态方程的数值解直接递推得到$X(t_k)$时刻的近似值。</p>
<h2 id="动态系统可控性和可测性"><a href="#动态系统可控性和可测性" class="headerlink" title="动态系统可控性和可测性"></a>动态系统可控性和可测性</h2><p>动态系统数学模型中的状态方程描述了控制输入量及初始状态对系统内部状态的影响，表明了系统内部结构特性，但不是所有的状态方程中的状态变量都受输入量的控制同时也不是所有系统的状态变量都能被观测到，由此引出了动态系统可控和可测性的问题。</p>
<p>1）可控性</p>
<p>在时间区间$[t_0,t_1]$，如果控制系统的输入$u(t)(t\in[t_0,t_1],t_1&gt;t_0)$可以将初始值$X(t_0)$转移到$X(t_1))$，$(X(t_1)$</p>
<p>为任意值），即系统的每一个状态变量都是可以控制的，这样的系统是完全可控的。</p>
<p>连续时不变动态系统的可控性条件为</p>
<script type="math/tex; mode=display">
rank[\mathop{B}\limits_{n×p} \quad \vdots\quad \mathop{AB}\limits_{n×p}\quad \vdots\quad \mathop{A^2B}\limits_{n×p}\quad \vdots\quad \cdots\quad \vdots\quad \mathop{A^{n-1}B}\limits_{n×p}\quad ]=n</script><p>离散线性时不变系统的可控性条件为</p>
<script type="math/tex; mode=display">
rank[\mathop{\Phi^0\Psi}\limits_{n×p} \quad \vdots\quad \mathop{\Phi\Psi}\limits_{n×p}\quad \vdots\quad \mathop{\Phi^2\Psi}\limits_{n×p}\quad \vdots\quad \cdots\quad \vdots\quad \mathop{\Phi^{k-1}\Psi}\limits_{n×p}\quad ]=n</script><p>2)可测性</p>
<p>可测性指能否从系统的输出中观测到系统内部信息的特性。在时间区间$[t_0,t_1]$，根据$t_0$到$t_1$的观测值$Z(t)(t\in[t_0,t_1],t_1&gt;t_0)$可以唯一地确定系统在初始时刻的状态$X(t_0)$,，则称系统是完全可测的。</p>
<p>连续时不变动态系统的可测性条件为</p>
<script type="math/tex; mode=display">
rank\begin{bmatrix}\mathop{H}\limits_{l×n}\\ \mathop{HA^1}\limits_{l×n} \\ \mathop{\vdots} \\ \mathop{H}\limits_{l×n}\mathop{A^{n-1}}\limits_{n×n} \end{bmatrix}=n</script><p>离散线性时不变系统的可测性条件为</p>
<script type="math/tex; mode=display">
rank\begin{bmatrix}\mathop{H}\limits_{l×n}\\ \mathop{H\Phi^1}\limits_{l×n} \\ \mathop{\vdots} \\ \mathop{H}\mathop{\Phi^{n-1}}\limits_{l×n} \end{bmatrix}=n</script>]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>最优估计学习备忘一</title>
    <url>/2020/03/12/%E6%9C%80%E4%BC%98%E4%BC%B0%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%A4%87%E5%BF%98%E4%B8%80/</url>
    <content><![CDATA[<h1 id="基础理论篇"><a href="#基础理论篇" class="headerlink" title="基础理论篇"></a>基础理论篇</h1><h2 id="最小二乘估计"><a href="#最小二乘估计" class="headerlink" title="最小二乘估计"></a>最小二乘估计</h2><p>1）最小二乘估计方法由德国数学家高斯提出，估计准则为残差（测绘中称为改正数）的平方和达到最小。假定有一个数学模型为</p>
<script type="math/tex; mode=display">
Z_{l×1}=H_{l×n}X_{n×1}+\nabla_{l×1} \quad (1.1)</script><script type="math/tex; mode=display">
E(Z)=HX \quad var(Z)=D \quad (1.2)</script><p>若现在通过某种方法得到参数估计$\hat{X}$，代入观测方程后可得到估计的观测值：<br><a id="more"></a></p>
<script type="math/tex; mode=display">
\hat{Z}_{l×1}=H_{l×n}\hat{X}_{n×1} \quad (1.3)</script><p>那么估计的观测值与实际观测值的差异为</p>
<script type="math/tex; mode=display">
v=\hat{Z}-Z=H\hat{X}-Z \quad (1.4)</script><p>其中</p>
<script type="math/tex; mode=display">
v=[v_1\quad v_2 \quad v_3 \quad \cdots \quad v_l]^T\quad (1.5)</script><p>上式中$v_i$表示观测值$Z_i$的残差，$v$也称为观测值的残差向量。要使观测值的残差平方和最小，即</p>
<script type="math/tex; mode=display">
\sum\limits_{i=1}^{i=l}v_i^2=min\quad (1.6)</script><p>向量的形式为</p>
<script type="math/tex; mode=display">
v^Tv=min\quad (1.7)</script><p>在方程（1.4）的无穷多组解中，能满足（1.6）的解即为最小二乘解$\hat{X}_{LS}$。</p>
<p>式1.6的准则认为所有观测值$Z_1,Z_2,\cdots,Z_l$对参数的影响相同，但有的时候观测值是不同精度的，所以在估计时我们希望精度好的观测值对参数估计的影响大，即方差小的观测值对参数估计的影响大，反之方差大的对参数估计的影响小，所以在式1.6的基础上根据观测值的方差赋予观测值一定的权重。权矩阵可设为</p>
<script type="math/tex; mode=display">
W=\sigma_0^2D^{-1}\quad (1.8)</script><p>，可证明$\sigma_0^2$的取值不影响最小二乘参数估计值，所以$\sigma_0^2$可以取任意实常数。亦可证明若观测值相互独立，$W$为对角矩阵</p>
<script type="math/tex; mode=display">
W=\begin{bmatrix}W_1& \quad & \quad & \quad \\
 \quad &W_2 &\quad  &\quad \\
\quad &\quad &\cdots &\quad \\
\quad &\quad &\quad &W_l \end{bmatrix}=\begin{bmatrix}\frac{\sigma_0^2}{\sigma_1^2}& \quad & \quad & \quad \\
 \quad &\frac{\sigma_0^2}{\sigma_2^2} &\quad  &\quad \\
\quad &\quad &\frac{\sigma_0^2}{\sigma_3^2} &\quad \\
\quad &\quad &\quad&\frac{\sigma_0^2}{\sigma_4^2} \end{bmatrix}\quad (1.9)</script><p>可以看到观测值的权与其方差成反比。</p>
<p>$W$为对角矩阵时最小二乘准则为</p>
<script type="math/tex; mode=display">
\sum\limits_{i=1}^{i=l}v_i^2W_i=min\quad (1.10)</script><p>最小二乘准则更一般的形式为</p>
<script type="math/tex; mode=display">
v^TWv=min\quad (1.11)</script><p>现建立满足最小二乘准则的目标方程</p>
<script type="math/tex; mode=display">
\Phi(\hat{X})=v^TWv=min\quad (1.12)</script><p>将误差方程1.4代入目标函数1.12</p>
<script type="math/tex; mode=display">
\Phi(\hat{X})=（H\hat{X}-Z）^TW（H\hat{X}-Z）=\hat{X}^TH^TWH\hat{X}-\hat{X}^TH^TWZ-Z^TWH\hat{X}+Z^TWZ\quad (1.13)</script><p>$\Phi(\hat{X})$是$\hat{X}$的函数，为了使其最小，由求函数极值方法（此方程可看成开口向上的二次函数，有极小值）得到</p>
<script type="math/tex; mode=display">
\frac{\partial\Phi(\hat{X})}{\partial\hat{X}}=2H^TWH\hat{X}-2H^TWZ=0\quad (1.14)</script><p>由于$H$为列满秩矩阵，$W$为对称方阵，所以$H^TWH$为满秩矩阵（可逆），方程有解且唯一，解得</p>
<script type="math/tex; mode=display">
\hat{X}_{LS}=(H^TWH)^{-1}H^TWZ\quad (1.15)</script><p>$\hat{X}_{LS}$即为满足最小二乘估计准则的解，式1.8代入式1.15得</p>
<script type="math/tex; mode=display">
\hat{X}_{LS}=(H^T\sigma_0^2D^{-1}H)^{-1}H^T\sigma_0^2D^{-1}Z=(H^TD^{-1}H)^{-1}H^TD^{-1}Z\quad (1.16)</script><p>由此证明$\sigma_0^2$的数值不影响$\hat{X}_{LS}$，$\hat{X}_{LS}$的值由设计矩阵$H$、观测值$Z$和观测值的方差矩阵$D$决定。</p>
<p>易证最小二乘估计为无偏估计且残差$v$与$\hat{X}_{LS}$不相关。</p>
<p>2）附有约束条件的最小二乘估计</p>
<p>线性化后的附有参数约束条件的函数模型为</p>
<script type="math/tex; mode=display">
\mathop{z}\limits_{l×1}=\mathop{H}\limits_{l×n}\mathop{x}\limits_{n×1}+\mathop{\Delta}\limits_{l×1}\\\mathop{C}\limits_{c×n}\mathop{x}\limits_{n×1}+\mathop{\varphi}\limits_{c×1}(X_0)=0</script><p>$H$为列满秩矩阵，$C$为行满秩矩阵，其秩为参数的约束条件的个数。</p>
<p>随机模型为</p>
<script type="math/tex; mode=display">
E(z)=HX \quad var(z)=D</script><p>现假设估计得到的$x$为$\hat{x}_{LS}$，那么误差修正后的观测值</p>
<script type="math/tex; mode=display">
\hat{z}=H\hat{x}_{LS}</script><p>观测值的残差为</p>
<script type="math/tex; mode=display">
v=H\hat{x}_{LS}-z</script><p>$\hat{x}_{LS}$同时满足约束条件</p>
<script type="math/tex; mode=display">
C\hat{x}_{LS}+\varphi(X_0)=0</script><p>联立上两式得到</p>
<script type="math/tex; mode=display">
\begin{bmatrix}I&-H\\0&C\end{bmatrix}\begin{bmatrix}v\\\hat{x}_{LS}\end{bmatrix}=\begin{bmatrix}-Z\\-\varphi(X_0)\end{bmatrix}</script><p>依题可知上式是有无穷组解的相容方程，现在要在这无穷多组解中找到能够满足最小二乘准则$v^TWv=min$并满足约束条件的一组解，按拉格朗日条件极值法构造方程</p>
<script type="math/tex; mode=display">
\Phi(\hat{x})=v^TWv+2K^T(Cx+\varphi(X_0))</script><p>约束条件代入上式可得</p>
<script type="math/tex; mode=display">
\Phi(\hat{x})=\hat{x}_{LS}^TH^TWH\hat{x}_{LS}-\hat{x}_{LS}^TH^TWz-z^TWH\hat{x}_{LS}+z^TWz+2K^T(Cx+\varphi(X_0))</script><p>上式对$x$求导并令其为零可得</p>
<script type="math/tex; mode=display">
H^TWH\hat{x}_{LS}-H^TWz+C^TK=0</script><p>与约束条件联立有</p>
<script type="math/tex; mode=display">
\begin{bmatrix}H^TWH&C^T\\C&0\end{bmatrix}\begin{bmatrix}\hat{x}_{LS}\\K\end{bmatrix}=\begin{bmatrix}H^TWz\\-\varphi(X_0)\end{bmatrix}</script><p>解得附有约束条件的最小二乘估计 $\hat{x}_{LS}$</p>
<script type="math/tex; mode=display">
\begin{cases}K=(C(H^TWH)^{-1}C^T)^{-1}(C(H^TWH)^{-1}HWz+\varphi(X_0))\\\hat{x}_{LS}=((H^TWH)^{-1}-(H^TWH)^{-1}C^T(C(H^TWH)^{-1}C^T)^{-1}C(H^TWH)^{-1})HWz\\-(H^TWH)^{-1}C^TC(H^TWH)^{-1}C^T)^{-1}\varphi(X_0)\end{cases}</script><p>3）递推最小二乘估计</p>
<p>上面介绍得两种最小二乘估计方法是集中所有观测值对参数进行估计得方法，即批处理方法。该方法会占用计算机的大量内存，不能实时对数据进行处理，解决这个问题的方法是采用最小二乘的递推算法。</p>
<p>设在第$k$次观测后，观测值$z_k$对应的误差方程为</p>
<script type="math/tex; mode=display">
V_k=H_k\hat{X}-Z_k</script><p>观测值向量$Z_k$的方差阵为$D_k$，权矩阵为$W_k=\sigma_0^2D_k$，因此这时参数的最小二乘解为</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k)}=(H_k^TW_kH_k)^{-1}H_k^TW_kZ_k</script><p>$k+1$次观测值向量为$z_{k+1}$，对应的误差方程为</p>
<script type="math/tex; mode=display">
v_{k+1}=h_{k+1}\hat{X}-z_{k+1}</script><p>观测值$z_{k+1}$的方差阵为$d_{k+1}$，权矩阵为$w_{k+1}=\sigma_0^2d_{k+1}^{-1}$。现集中所有的观测值，数量为$l_{k+1}$，构成观测值向量$Z_{k+1}=[Z_k \quad z_{k+1}]^T$，其对应的误差方程为</p>
<script type="math/tex; mode=display">
V_{k+1}=H_{k+1}\hat{X}-Z_{k+1}</script><p>其中</p>
<script type="math/tex; mode=display">
V_{k+1}=\begin{bmatrix}V_k\\v_{k+1}\end{bmatrix} \quad H_{k+1}=\begin{bmatrix}H_k\\h_{k+1}\end{bmatrix} \quad Z_{k+1}=\begin{bmatrix}Z_k\\z_{k+1}\end{bmatrix}</script><p>观测值向量$Z_{k+1}$对应的权矩阵为</p>
<script type="math/tex; mode=display">
\quad W_{k+1}=\begin{bmatrix}W_k& \quad\\ \quad&w_{k+1}\end{bmatrix}</script><p>以上是批处理算法的数学模型，其参数估计为</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k+1)}=(H_{k+1}^TW_{k+1}H_{k+1})^{-1}H_{k+1}^TW_{k+1}Z_{k+1}\\=(H_k^TW_kH_k+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}(H_k^TW_kZ_k+h_{k+1}^Tw_{k+1}z_{k+1})</script><p>又$\hat{X}_{B(k+1)}$的协因数矩阵为</p>
<script type="math/tex; mode=display">
Q_{\hat{X}_{B(k+1)}}=(H_k^TW_kH_k+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}=(Q_{\hat{X}_{B(k)}}^{-1}+h_{k+1}^Tw_{k+1}h_{k+1})^{-1}</script><p>代入上一式可得</p>
<script type="math/tex; mode=display">
\hat{X}_{B(k+1)}=\hat{X}_{B(k)}+Q_{\hat{X}_{B(k+1)}}h_{k+1}^Tw_{k+1(z_{k+1}-h_{k+1}\hat{X}_{B(k)})}</script><p>上式即为递推最小二乘估计的参数估计式。</p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>极大似然估计提供了一种给定观测值来评估模型参数的方法，即“模型已定，参数位置”，为了能够估计得到分布中的未知参数$x$，一个自然的想法是一组未知参数的估值$\hat{x}$决定的概率密度函数$p(z/\hat{x})$最大，这意味着这组观测值在这个参数估计下出现的”可能性“的概率最大，即</p>
<script type="math/tex; mode=display">
L(x)=p(z/\hat{x})=max</script><p>基于这个准则的方法称为极大似然估计，$L(x)$称为似然函数，它是参数$x$的条件分布。求总体参数$x$的极大似然估计值的问题就是求似然函数$L(x)$的最大值问题。对似然函数求导</p>
<script type="math/tex; mode=display">
\frac{\partial p(z/x)}{\partial x}\mid_{x=\hat{x}_{ML}} =0</script><p>等价于（对数函数单增，两式极值点一致）</p>
<script type="math/tex; mode=display">
\frac{\partial lnp(z/x)}{\partial x}\mid_{x=\hat{x}_{ML}} =0</script><p>（1）极大似然函数估计需要已知观测值的概率密度函数即通过已知观测值与未知参数的条件分布来建立似然函数，而最小二乘估计只需要知道观测值与参数的函数关系和观测值的特征值；</p>
<p>（2）极大似然估计并不考虑参数的先验分布，也就是说极大似然估计中，将$X$视为随机变量；</p>
<p>（3）极大似然估计并不是线性估计但当观测值服从正态分布并且观测值与参数之间是线性关系时，极大似然估计与最小二乘估计等价；</p>
<h2 id="极大验后估计"><a href="#极大验后估计" class="headerlink" title="极大验后估计"></a>极大验后估计</h2><p>极大似然估计是以”$L(x)=p(z/\hat{x})=max$“为估计准则，在其估计中并没有考虑参数$X$的先验信息。极大验后估计考虑到了$X$的先验信息是使验后条件概率密度函数$p(x\mid z)$最大的$\hat{x}$为参数的估计，即</p>
<script type="math/tex; mode=display">
p(x\mid z)=max</script><p>它的含义是，给定一组观测值$Z$，在这组观测值的条件下使得$x$出现的概率最大，故极大验后估计满足</p>
<script type="math/tex; mode=display">
\frac{\partial lnp(z\mid x)}{\partial x}\mid_{x=\hat{x}_{MAP}} =0</script><h2 id="最小（均）方差估计"><a href="#最小（均）方差估计" class="headerlink" title="最小（均）方差估计"></a>最小（均）方差估计</h2><p>1）均方差（MSE）反映的是估计值与其真值之间的密集程度或者估计值的真误差在零附近的密集程度，这里的方差估计是无偏估计，均方差就是方差，故称为最小方差估计。最小方差估计的准则为</p>
<script type="math/tex; mode=display">
J_0(\hat{X}(Z))=E[(\hat{X}-X)(\hat{X}-X)^T]=min</script><p>式中，$\hat{X}$是通过观测值$Z$得到的对$X$的估计，它是$Z$的函数，所以$(\hat{X}-X)(\hat{X}-X)^T$是$X$和$Z$的函数，假设$f(x,z)$为$X$和$Z$的联合分布，那么</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\iint(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dxdz</script><p>$\because$    $f(z)\geq0$恒成立，故上式等价于</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\int(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dx=min</script><p>解得</p>
<script type="math/tex; mode=display">
E(X/Z)-\hat{x}=0</script><p>即最小方差准则下的参数估计为</p>
<script type="math/tex; mode=display">
\hat{X}_{MV}=E(X/Z)</script><p>若把最小方差估计的准则改为使$J_0(\hat{X}(Z))$的迹最小，即</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}(Z)))=E[(\hat{X}-X)(\hat{X}-X)^T]=min</script><p>即</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}(Z)))=\int(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dx\\=\hat{x}^T\hat{x}+\int x^Txf(x/z)dx-2\hat{x}^T\int xf(x/z)dx=min</script><p>上式对$\hat{x}$求导并零其为零得到</p>
<script type="math/tex; mode=display">
2\hat{x}-2\int xf(x/z)=0</script><p>因为$\int xf(x/z)dx=E(X/Z)$，所以方差的迹最小的解为</p>
<script type="math/tex; mode=display">
\hat{X}_{MV}=E(X/Z)</script><p>即方差迹最小的解与方差最小的解结果完全相同，所以可以用方差迹最小的准则来代替方差最小准则来得到方差最小的解。</p>
<p>对于任何分布而言，参数$X$的最小方差估计$\hat{X}_{MV}$都为其验后期望，在正态分布的情况下，极大验后估计也为$E(X/Z)$，即极大验后估计与最小方差估计等价。</p>
<p>2）线性最小方差估计</p>
<p>线性最小方差估计是一种特殊的最小方差估计，它是指估计值$\hat{X}$是观测量$Z$的线性函数，并使得估计的均方差最小的估计，即估计量具有如下形式</p>
<script type="math/tex; mode=display">
\mathop{\hat{X}}\limits_{n×1}=\mathop{a_L}\limits_{n×1}+\mathop{B_L}\limits_{n×l}\mathop{Z}\limits_{l×1}</script><p>同时估计量</p>
<script type="math/tex; mode=display">
\hat{X}=MSE(\hat{X})=E[(X-\hat{X})(X-\hat{X})^T]=min</script><p>由上面对最小方差估计得推导得$J_0(\hat{X})=min$等价于</p>
<script type="math/tex; mode=display">
tr(J_0(\hat{X}))=E[(X-a_L-B_LZ)^T(X-a_L-B_LZ)]=min</script><p>根据极值理论将$tr(J_0(\hat{X}))$对$a_L$和$B_L$分别求偏导并令其为零</p>
<script type="math/tex; mode=display">
E(X-a_L-B_LZ)=0\\E((X-a_L-B_LZ)^TZ)=0</script><p>上两式联立解得</p>
<script type="math/tex; mode=display">
\begin{cases}a_L=E(X)-D_{XZ}D_Z^{-1}E(Z)\\B_L=D_{XZ}D_Z^{-1}\end{cases}</script><p>$a_L$和$B_L$表示满足线性最小方差的解，最后将解代入估计式得到最小方差估计</p>
<script type="math/tex; mode=display">
\hat{X}_L=a_L+B_LZ=E(X)+D_{XZ}D_Z^{-1}(Z-E(Z))</script><p>线性最小方差估计与最小二乘估计一样并不需要知道随机变量具体分布，但线性最小方差估计利用了参数$X$的先验随机信息$\mu_x$（期望）和$D_x$（方差），也就是说当有参数$X$的先验随机信息时，线性最小方差估计改进了最小二乘估计。</p>
<h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>在估计某个量时，随机误差的干扰使估计产生误差</p>
<script type="math/tex; mode=display">
\nabla X=X-\hat{X}(Z)</script><p>这种差异造成的损失可以用损失函数来描述，即损失函数为</p>
<script type="math/tex; mode=display">
L(\nabla X)=L(X,\hat{X}(Z))</script><p>如何来量化着这种损失有不同的定义，一般而言，估计误差越大，损失就越大，典型的损失函数有：</p>
<p>平方损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=(X-\hat{X})(X-\hat{X})^T</script><p>绝对值损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=\mid X-\hat{X}(Z)\mid</script><p>均值损失函数</p>
<script type="math/tex; mode=display">
L(X,\hat{X}(Z))=\begin{cases}0 \quad (\mid\nabla X\mid\leq\frac{\nabla}{2})\\ 1 \quad (\mid\nabla X\mid＞\frac{\nabla}{2})\end{cases}</script><p>损失函数在随机变量的整个域内的平均值，即期望，定义为贝叶斯风险</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=\int_{-∞}^{+∞}\int_{-∞}^{+∞}L(X,\hat{X}(Z))f(x,z)dxdz</script><p>贝叶斯估计就是得到使上述的风险值最小的估计</p>
<script type="math/tex; mode=display">
\hat{X}_B(Z)=argmin(R_B(X,\hat{X}(Z)))</script><p>即</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=min</script><p>下面根据不同的损失函数来推导贝叶斯估计</p>
<p>1）平方损失函数</p>
<p>平方损失函数的贝叶斯风险</p>
<script type="math/tex; mode=display">
R_B(X,\hat{X}(Z))=\int_{-∞}^{+∞}\int_{-∞}^{+∞}(X-\hat{X})(X-\hat{X})^Tf(x,z)dxdz</script><p>将上式与</p>
<script type="math/tex; mode=display">
J_0(\hat{X})=\iint(\hat{x}-x)(\hat{x}-x)^Tf(x,z)dxdz</script><p>比较可知平方损失函数的贝叶斯风险就是其均方差，所以贝叶斯风险最小的参数估计</p>
<script type="math/tex; mode=display">
\hat{X}_B(Z)=\hat{X}_{MV}=E(X/Z)</script><p>2)绝对损失函数</p>
<p>绝对损失函数的验后风险为</p>
<script type="math/tex; mode=display">
r_{abs}(\hat{X}(Z)/Z)=\int_{-∞}^{+∞}\mid x-\hat{x}(z) \mid f(x/z)dx\\=\int_{-∞}^{\hat{x}(z)}(\hat{x}(z)-x )f(x/z)dx+\int_{\hat{x}(z)}^{+∞}( \hat{x}(z)-x) f(x/z)dx=min</script><p>对上式求导，并令导数在$\hat{X}(z)=\hat{X}_{abs}(z)$处为零，得</p>
<script type="math/tex; mode=display">
\int_{-∞}^{\hat{x}_{abs}(z)}f(x/z)dx=\int_{\hat{x}_{abs}(z)}^{+∞}f(x/z)dx</script><p>由上式可知，贝叶斯估计为条件概率密度$f_{X/Z(x/z)}$得中位数，即$\hat{X}_{abs}(z)=\hat{X}_{med}$，所以也称为条件中位数估计，记为$\hat{X}_{med}$。</p>
<p>3）均匀损失价函数</p>
<p>均匀损失价函数验后风险为</p>
<script type="math/tex; mode=display">
r_{unf}(\hat{X}(Z)/Z)=\int_{-∞}^{\hat{X}(Z)-\frac{\Delta}{2}} f(x/z)dx+\int_{\hat{X}(Z)+\frac{\Delta}{2}}^{+∞} f(x/z)dx =1-\int_{\hat{X}(Z)-\frac{\Delta}{2}}^{\hat{X}(Z)+\frac{\Delta}{2}} f(x/z)dx=min</script><p>上式等价于</p>
<script type="math/tex; mode=display">
f_{X/Z}(x/z) \mid_{x=\hat{x}}=max</script><p>即贝叶斯估计为极大验后估计$\hat{X}_{MAP}$，由于$f_{X/Z}(x/z)$不容易得到，由条件概率公式可知上式也等价于</p>
<script type="math/tex; mode=display">
f_{Z/X}(z/x) ×f_x(x)\mid_{x=\hat{x}_{MAP}}=max</script><h2 id="各个估计方法的总结对比"><a href="#各个估计方法的总结对比" class="headerlink" title="各个估计方法的总结对比"></a>各个估计方法的总结对比</h2><p>（1）最小二乘估计不需要知道随机变量得具体分布，只需要已知观测值与参数得关系和观测值得方差，将估计参数$X$视为非随机变量，其估计是线性估计。</p>
<p>（2）极大似然估计需要知道参数条件下的观测值的具体分布$p(z/x)$，在其估计中并没有考虑$X$的先验分布，即视$X$为非随机变量。当观测值与参数呈线性关系并且正态分布时，极大似然估计与最小二乘估计等价。</p>
<p>（3）极大验后估计需要已知$p(z \mid x)$和$p(x)$。与极大似然估计比较，极大验后估计用了$X$的先验分布$p(x)$，所以改进了极大似然估计。当$p(z,x)$服从正态分布时，其估计值为$E(X/Z)$，这与最小方差估计等价。</p>
<p>（4）最小方差估计需要已知分布$p(x\mid z)$或者$E(X/Z)$，其估计值为$E(X/Z)$，与极大验后估计一样，最小方差估计利用了$X$的先验随机信息。</p>
<p>（5）线性最小方差估计需要在最小方差的准则上满足其估计与观测值呈线性关系。线性最小方差估计对已知条件要求较为宽松，只需要知道观测值$Z$和参数$X$的期望、方差、协方差。</p>
<p>（6）贝叶斯估计使估计误差造成的损失在整个验后分布的平均值最小，即损失函数的验后期望最小。不同的损失函数得到不同的估计，如当损失函数为平方损失函数时，其估计与最小方差估计等价；当损失函数为均匀损失函数时，其估计与最大验后估计等价。</p>
<p>（7）最小二乘估计和线性最小方差估计为线性估计，其它方法只有在观测值与参数呈线性关系，并且是正态分布时才是线性估计。</p>
]]></content>
      <categories>
        <category>最优估计</category>
      </categories>
      <tags>
        <tag>最优估计</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/03/11/hello-world/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
